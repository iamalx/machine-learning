{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:SteelBlue\">Lesson:</span> Building an ML Pipeline</h1>\n",
    "<hr>\n",
    "We've talked about the process of training a model already which is a failry straight forward process when were talking about a single model, with predefined HyperParameters.\n",
    "\n",
    "However, in most real life situations we DO NOT know what models may be the best suited for the task at hand, let alone the appropriate hyperparameters for each model. \n",
    "\n",
    "**The Solution?**\n",
    "\n",
    "We will write a series of loops that use *Cross-Validation* to \"test\" each model, including every combination of relevant hyperparameters —for each model, in order to discern which configuration is the most effective.\n",
    "\n",
    "**Relevant topics for this section on Building Pipelines:**\n",
    "1. Dictionaries\n",
    "2. Looping\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PyData Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy for numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# Pickle for saving model files\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import SKLearn Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Logistic Regression from sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import RandomForestClassifier and GradientBoostingClassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries to make Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import make_pipeline from sklearn.pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# import StandardScaler from sklearn.preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import GridSearchCV from sklearn.model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# import roc_curve and auc from sklearn.metrics\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `read_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./pima_indian_diabetes/data/pima_diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset into `X` and `y` sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Outcome'], axis=1)\n",
    "y = df['Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `shape` of `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `shape` of `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split `X` and `y` into Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We want to split both the `X` and `y` datasets into Train and Test sets. This will result in 4 datasets total.**\n",
    "\n",
    "```(X_train, X_test, y_train, y_test)```\n",
    "\n",
    "> **`X`:** `X_train` and `X_test`\n",
    "\n",
    "> **`y`:** `y_train` and `y_test`\n",
    "\n",
    "The **Train Set** is the set that we will operform further splits on, during the cross-validation step. The motivation for this is to find the best combination of model and hyperparameters, which we can only achieve by running performance metrics on every possible combination of Model and Hyperparameters.\n",
    "\n",
    "The **Test Set** will remain untouched until after the cross-validation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `train_test_split()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you will pass in the following values to `train_test_split`\n",
    "```python \n",
    "X\n",
    "y\n",
    "test_size = 0.2, \n",
    "stratify = df['Outcome'], \n",
    "random_state = 123\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify = df['Outcome'], \n",
    "    random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `print()` `shape` of `X_train` and `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (614, 8)\n",
      "X_test: (154, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train:\", X_train.shape) \n",
    "print(\"X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `print()` `shape` of `y_train` and `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: (614,)\n",
      "y_test: (154,)\n"
     ]
    }
   ],
   "source": [
    "print(\"y_train:\", y_train.shape) \n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both **Train** sets, **X_train and y_train**, should have the **same number of rows**\n",
    "\n",
    "Both **Test** sets, **X_test and y_test**, should also have the **same number of rows**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Pipeline Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline dictionary can contain as many models as your heart's content. We use this approach to automate the process of fitting a model, predicting labels, and finally testing model accuracy.\n",
    "\n",
    "This will all be done with the SKlearn model_selection function **GridSearchCV()** which expects at least two arguments a **Pipeline** dictionary and a **Hyperparameters** dictionary, with matching keys!!!\n",
    "\n",
    "Let's start with the **Pipeline Dictionary!**\n",
    "\n",
    "The `keys` should contain the model name's —as `strings`\n",
    "The `values` should contain pipeline objects.\n",
    ">The `pipeline` objects should contain a normalizer/standardizer, and instantiate a model.\n",
    "\n",
    "<br>\n",
    "\n",
    "`\"model_name\": make_pipeline(scalar()/normalizer(), ModelRegressor()/ModelClassifier())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict = {\n",
    "    'l1': make_pipeline(StandardScaler(), LogisticRegression(penalty= 'l1', random_state= 123)),\n",
    "    'l2': make_pipeline(StandardScaler(), LogisticRegression(penalty= 'l2', random_state= 123)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "\n",
    "## <span style=\"color:RoyalBlue\">Exercise 1</span> - Add Pipelines to `Pipeline_dict`\n",
    "\n",
    "Add the `pipelines` for the three remaining Classifiers: `RandomForrestClassifier`, `GradientBoostClassifier`, and `KNeighborsClassifier`.\n",
    "\n",
    "> 1. All `Pipelines` in `pipeline_dict` should begin with a `StandrardScaler()`\n",
    "> 2. All `ModelInstantiations()` should set a `random_state = 123`, except for `KNeighborsClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do work here\n",
    "pipeline_dict['rf'] = make_pipeline(StandardScaler(), RandomForestClassifier(random_state= 123))\n",
    "pipeline_dict['gb'] = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=123))\n",
    "pipeline_dict['kn'] = make_pipeline(StandardScaler(), KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your work! \n",
    "All `pipeline_dict` `keys` should contain a `sklearn.pipeline.Pipeline` object as a matching `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: <class 'sklearn.pipeline.Pipeline'>\n",
      "l2: <class 'sklearn.pipeline.Pipeline'>\n",
      "rf: <class 'sklearn.pipeline.Pipeline'>\n",
      "gb: <class 'sklearn.pipeline.Pipeline'>\n",
      "kn: <class 'sklearn.pipeline.Pipeline'>\n"
     ]
    }
   ],
   "source": [
    "for key, val in pipeline_dict.items():\n",
    "    print(key + \":\", type(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build a Hyperparameters Nested-Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following describes the goal for this section, however we will begin with some exercises to help understand the concept of a nested dictionary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Final Hyperparameter Dictionary: `hp_dict`**\n",
    "\n",
    "> Every `key` in `hp_dict` will match the keys from `pipeline_dict`.\n",
    "\n",
    "> Every `value` in `hp_dict` will be a dictionary.\n",
    "\n",
    "**Nested Dictionaries inside `hp_dict`**\n",
    "\n",
    "> Every `key` in each nested `dict` will be a different hyperparameter that needs tuning.\n",
    "\n",
    "> Every `value` will hold a range of values that each particular hyperparameter can take on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of an `hp_dict` with values for 2 different models**\n",
    "\n",
    "*Notice that the `values` of `hp_dict` are surrounded by `{}`, this means that they are themselves dictionaries!*\n",
    "```python \n",
    "hp_dict = {\n",
    "    'kn':{\"kneighborsclassifier__n_neighbors\" : np.arange(1,11)}\n",
    "    'l1':{\"logisticregression__C\" : np.linspace(1e-4, 1e4, 10)}\n",
    "}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "\n",
    "## <span style=\"color:RoyalBlue\">Exercise 2</span> - Performing Action during Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loop through `pipeline_dict` to print out the `type` of object that is associated with the `get_params()` method, for every model \"key.\"**\n",
    "\n",
    "**Remember:** Dictionaries in Python are set up like this:\n",
    "```python \n",
    "some_dict = {\n",
    "    key1 : val1\n",
    "    key2 : val2\n",
    "}```\n",
    "\n",
    "\n",
    "**Hint:** I want you to **`print` every `key`** and the **`type` of object that results from calling the `get_params()` method on every matching `value` of `pipeline_dict`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 <class 'dict'>\n",
      "l2 <class 'dict'>\n",
      "rf <class 'dict'>\n",
      "gb <class 'dict'>\n",
      "kn <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "for key, value in pipeline_dict.items():\n",
    "    print(key, type(value.get_params()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What type of object does get_params() return? \n",
    "What could we do to said object?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "## <span style=\"color:RoyalBlue\">Exercise 3</span> - Nested Dictionary Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since `get_params()` results in a `dict` object we can use `.items()` to loop through every `key` `value` pair inside of each parameter `dict`!**\n",
    "\n",
    "There is nothing stopping us from nesting a second for loop from inside a for loop!\n",
    "\n",
    "This is one of the harder concepts to understand at first, but once you get it, you'll realize how straightforward the process really is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1\n",
      "-------\n",
      "memory None\n",
      "steps [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=123, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))]\n",
      "standardscaler StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "logisticregression LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=123, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "standardscaler__copy True\n",
      "standardscaler__with_mean True\n",
      "standardscaler__with_std True\n",
      "logisticregression__C 1.0\n",
      "logisticregression__class_weight None\n",
      "logisticregression__dual False\n",
      "logisticregression__fit_intercept True\n",
      "logisticregression__intercept_scaling 1\n",
      "logisticregression__max_iter 100\n",
      "logisticregression__multi_class warn\n",
      "logisticregression__n_jobs None\n",
      "logisticregression__penalty l1\n",
      "logisticregression__random_state 123\n",
      "logisticregression__solver warn\n",
      "logisticregression__tol 0.0001\n",
      "logisticregression__verbose 0\n",
      "logisticregression__warm_start False\n",
      "\n",
      "l2\n",
      "-------\n",
      "memory None\n",
      "steps [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=123, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False))]\n",
      "standardscaler StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "logisticregression LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=123, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "standardscaler__copy True\n",
      "standardscaler__with_mean True\n",
      "standardscaler__with_std True\n",
      "logisticregression__C 1.0\n",
      "logisticregression__class_weight None\n",
      "logisticregression__dual False\n",
      "logisticregression__fit_intercept True\n",
      "logisticregression__intercept_scaling 1\n",
      "logisticregression__max_iter 100\n",
      "logisticregression__multi_class warn\n",
      "logisticregression__n_jobs None\n",
      "logisticregression__penalty l2\n",
      "logisticregression__random_state 123\n",
      "logisticregression__solver warn\n",
      "logisticregression__tol 0.0001\n",
      "logisticregression__verbose 0\n",
      "logisticregression__warm_start False\n",
      "\n",
      "rf\n",
      "-------\n",
      "memory None\n",
      "steps [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestclassifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=123, verbose=0, warm_start=False))]\n",
      "standardscaler StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "randomforestclassifier RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=123, verbose=0, warm_start=False)\n",
      "standardscaler__copy True\n",
      "standardscaler__with_mean True\n",
      "standardscaler__with_std True\n",
      "randomforestclassifier__bootstrap True\n",
      "randomforestclassifier__class_weight None\n",
      "randomforestclassifier__criterion gini\n",
      "randomforestclassifier__max_depth None\n",
      "randomforestclassifier__max_features auto\n",
      "randomforestclassifier__max_leaf_nodes None\n",
      "randomforestclassifier__min_impurity_decrease 0.0\n",
      "randomforestclassifier__min_impurity_split None\n",
      "randomforestclassifier__min_samples_leaf 1\n",
      "randomforestclassifier__min_samples_split 2\n",
      "randomforestclassifier__min_weight_fraction_leaf 0.0\n",
      "randomforestclassifier__n_estimators warn\n",
      "randomforestclassifier__n_jobs None\n",
      "randomforestclassifier__oob_score False\n",
      "randomforestclassifier__random_state 123\n",
      "randomforestclassifier__verbose 0\n",
      "randomforestclassifier__warm_start False\n",
      "\n",
      "gb\n",
      "-------\n",
      "memory None\n",
      "steps [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('gradientboostingclassifier', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=123,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False))]\n",
      "standardscaler StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "gradientboostingclassifier GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=123,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "standardscaler__copy True\n",
      "standardscaler__with_mean True\n",
      "standardscaler__with_std True\n",
      "gradientboostingclassifier__criterion friedman_mse\n",
      "gradientboostingclassifier__init None\n",
      "gradientboostingclassifier__learning_rate 0.1\n",
      "gradientboostingclassifier__loss deviance\n",
      "gradientboostingclassifier__max_depth 3\n",
      "gradientboostingclassifier__max_features None\n",
      "gradientboostingclassifier__max_leaf_nodes None\n",
      "gradientboostingclassifier__min_impurity_decrease 0.0\n",
      "gradientboostingclassifier__min_impurity_split None\n",
      "gradientboostingclassifier__min_samples_leaf 1\n",
      "gradientboostingclassifier__min_samples_split 2\n",
      "gradientboostingclassifier__min_weight_fraction_leaf 0.0\n",
      "gradientboostingclassifier__n_estimators 100\n",
      "gradientboostingclassifier__n_iter_no_change None\n",
      "gradientboostingclassifier__presort auto\n",
      "gradientboostingclassifier__random_state 123\n",
      "gradientboostingclassifier__subsample 1.0\n",
      "gradientboostingclassifier__tol 0.0001\n",
      "gradientboostingclassifier__validation_fraction 0.1\n",
      "gradientboostingclassifier__verbose 0\n",
      "gradientboostingclassifier__warm_start False\n",
      "\n",
      "kn\n",
      "-------\n",
      "memory None\n",
      "steps [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('kneighborsclassifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'))]\n",
      "standardscaler StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "kneighborsclassifier KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "standardscaler__copy True\n",
      "standardscaler__with_mean True\n",
      "standardscaler__with_std True\n",
      "kneighborsclassifier__algorithm auto\n",
      "kneighborsclassifier__leaf_size 30\n",
      "kneighborsclassifier__metric minkowski\n",
      "kneighborsclassifier__metric_params None\n",
      "kneighborsclassifier__n_jobs None\n",
      "kneighborsclassifier__n_neighbors 5\n",
      "kneighborsclassifier__p 2\n",
      "kneighborsclassifier__weights uniform\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in pipeline_dict.items():\n",
    "    print(key)\n",
    "    print(\"-------\")\n",
    "    for k, v in value.get_params().items():\n",
    "        print(k, v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "## <span style=\"color:RoyalBlue\">Exercise 4</span> - Making individual HyperParameter Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for L1 aka \"LASSO\" Logistic Regression\n",
    "We want to tune:\n",
    "1. C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should be `10` *equally spaced values* between \n",
    "> `1e-4` or 0.0001 and `1e4` or 10,000\n",
    "\n",
    "**Hint**: What numpy *generator* function allows us to create a range of **equally spaced values**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linspace(1e-4, 1e4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_hyperparameters = { \n",
    "    \"logisticregression__C\" : np.linspace(1e-4, 1e4, 10) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for L2 aka \"Ridge\" Logistic Regression\n",
    "We want to tune:\n",
    "1. C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linspace(1e-4, 1e4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_hyperparameters = { \n",
    "    \"logisticregression__C\" : np.linspace(1e-4, 1e4, 10) \n",
    "}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for Random Forest Classifier\n",
    "We want to tune:\n",
    "1. n_estimators\n",
    "2. max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_hyperparameters = { \n",
    "        \"randomforestclassifier__n_estimators\" : [10, 100, 200], \n",
    "        \"randomforestclassifier__max_features\" : [\"auto\", \"sqrt\",\"log2\",None]  \n",
    "}     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for Gradient Boosting Classifier\n",
    "We want to tune:\n",
    "1. n_estimators\n",
    "2. learning_rate\n",
    "3. max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_hyperparameters = {\n",
    "        \"gradientboostingclassifier__n_estimators\" : [10, 100, 200],\n",
    "        \"gradientboostingclassifier__learning_rate\" : [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "        \"gradientboostingclassifier__max_depth\" : [1, 3, 5, 7, 9]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for KNN Classifier\n",
    "We want to tune:\n",
    "1. n_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.arange(1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_hyperparameters = { \"kneighborsclassifier__n_neighbors\" : \n",
    "                      np.arange(1,11)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "## <span style=\"color:RoyalBlue\">Exercise 5a</span> - Assemble `hp_dict`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the final `hp_dict` with similar naming convention to `pipeline_dict`\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_dict = {\n",
    "    'l1' : l1_hyperparameters,\n",
    "    'l2' : l2_hyperparameters,\n",
    "    'kn' : kn_hyperparameters,\n",
    "    'gb' : gb_hyperparameters,\n",
    "    'rf' : rf_hyperparameters,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:RoyalBlue\">Exercise 5b</span> - Loop throu `hp_dict` to verify work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a for loop that prints out each `key` and `value` in `hp_dict`, no need to make it look fancy like me, can be done in 3 lines of code. My example took 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1\n",
      "------\n",
      "logisticregression__C [1.0000000e-04 1.1111112e+03 2.2222223e+03 3.3333334e+03 4.4444445e+03\n",
      " 5.5555556e+03 6.6666667e+03 7.7777778e+03 8.8888889e+03 1.0000000e+04]\n",
      "\n",
      "l2\n",
      "------\n",
      "logisticregression__C [1.0000000e-04 1.1111112e+03 2.2222223e+03 3.3333334e+03 4.4444445e+03\n",
      " 5.5555556e+03 6.6666667e+03 7.7777778e+03 8.8888889e+03 1.0000000e+04]\n",
      "\n",
      "kn\n",
      "------\n",
      "kneighborsclassifier__n_neighbors [ 1  2  3  4  5  6  7  8  9 10]\n",
      "\n",
      "gb\n",
      "------\n",
      "gradientboostingclassifier__n_estimators [10, 100, 200]\n",
      "gradientboostingclassifier__learning_rate [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
      "gradientboostingclassifier__max_depth [1, 3, 5, 7, 9]\n",
      "\n",
      "rf\n",
      "------\n",
      "randomforestclassifier__n_estimators [10, 100, 200]\n",
      "randomforestclassifier__max_features ['auto', 'sqrt', 'log2', None]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, val in hp_dict.items():\n",
    "    print(key)\n",
    "    print('------')\n",
    "    for k, v in val.items():\n",
    "        print(k, v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both a `Pipeline_dict` and a `hp_dict` we can build the loop that will handle cross-validation for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "## <span style=\"color:RoyalBlue\">Exercise 6</span> - Write a loop that uses `GridSearchCV()` to fit models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 has been fitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2 has been fitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf has been fitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb has been fitted\n",
      "kn has been fitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "# Create empty dictionary called fitted_models\n",
    "fitted_models = {}\n",
    "\n",
    "# Loop through model pipelines, tuning each one and saving it to fitted_models\n",
    "for name, pipeline in pipeline_dict.items():\n",
    "    \n",
    "    # Create cross-validation object from pipeline and hyperparameters\n",
    "    model = GridSearchCV(pipeline, hp_dict[name], cv=10, n_jobs=-1)\n",
    "    \n",
    "    # Fit model on X_train, y_train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Store model in fitted_models[name] \n",
    "    fitted_models[name] = model\n",
    "    \n",
    "    # Print '{name} has been fitted'\n",
    "    print(name, 'has been fitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate metrics\n",
    "\n",
    "Finally, it's time to evaluate our models and pick the best one.\n",
    "\n",
    "<br>\n",
    "\n",
    "**First, display the <code style=\"color:steelblue\">best\\_score_</code> attribute for each fitted model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "## <span style=\"color:RoyalBlue\">Exercise 7</span> - Access `best_score_` attribute for each cross-validation object in `fitted_models`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint: this is what it looks like to access the `best_score_` attribute for a single member of `fitted_models`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7736156351791531"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_models['l1'].best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your for-loop here! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 0.7736156351791531\n",
      "l2 0.7736156351791531\n",
      "rf 0.757328990228013\n",
      "gb 0.7654723127035831\n",
      "kn 0.737785016286645\n"
     ]
    }
   ],
   "source": [
    "for model_name, cv_obj in fitted_models.items():\n",
    "    print(model_name, cv_obj.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
