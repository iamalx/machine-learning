{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rn9OYaLpaRgp"
   },
   "source": [
    "<h1 style=\"font-size:42px; text-align:center; margin-bottom:30px;\"><span style=\"color:SteelBlue\">Regression:</span> Model Training</h1>\n",
    "<hr>\n",
    "\n",
    "At last, it's time to build our models! \n",
    "\n",
    "It might seem like it took us a while to get here, but professional data scientists actually spend the bulk of their time on the 3 steps leading up to this one: \n",
    "1. Exploratory Analysis\n",
    "2. Data Cleaning\n",
    "3. Feature Engineering\n",
    "\n",
    "That's because the biggest jumps in model performance are from **better data**, not from fancier algorithms.\n",
    "\n",
    "<br><hr id=\"toc\">\n",
    "\n",
    "### In this lesson...\n",
    "\n",
    "First, we'll load our analytical base table from lesson 3. \n",
    "\n",
    "Then, we'll go through the essential modeling steps:\n",
    "\n",
    "1. [Split your dataset](#split)\n",
    "2. [Build model pipelines](#pipelines)\n",
    "3. [Declare hyperparameters to tune](#hyperparameters)\n",
    "4. [Fit and tune models with cross-validation](#fit-tune)\n",
    "5. [Evaluate metrics and select winner](#evaluate)\n",
    "\n",
    "Finally, we'll save the best model as a project deliverable!\n",
    "\n",
    "<br><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oH2NNwaQaRgr"
   },
   "source": [
    "### First, let's import libraries, recruit models, and load the analytical base table.\n",
    "\n",
    "Let's import our libraries and load the dataset. It's good practice to keep all of your library imports at the top of your notebook or program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FBHUjqaeaRgt"
   },
   "outputs": [],
   "source": [
    "# NumPy for numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for DataFrames\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Matplotlib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "# display plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-Learn for Modeling\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UrzOhCuwaRg-"
   },
   "source": [
    "Next, let's import 5 algorithms we introduced in the previous lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XSDKE53caRg_"
   },
   "outputs": [],
   "source": [
    "# Import ElasticNet, Ridge, and Lasso Regression from sklearn.linear_model\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso\n",
    "\n",
    "# Import RandomForest and GradientBoosting Regressors from sklearn.ensemble\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVcRrJ4uaRhC"
   },
   "source": [
    "<strong>Quick note about this lesson.</strong><br> In this lesson, we'll be relying heavily on Scikit-Learn, which has many helpful functions we can take advantage of. However, we won't import everything right away. Instead, we'll be importing each function from Scikit-Learn as we need it. That way, we can point out where you can find each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FmuNqawOaRhE",
    "outputId": "ddafd97b-3c15-43fb-af50-3d95151c7538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1863, 41)\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned dataset from lesson 3\n",
    "df = pd.read_csv('project_files/real-estate_abt.csv')\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tx_price</th>\n",
       "      <th>beds</th>\n",
       "      <th>baths</th>\n",
       "      <th>sqft</th>\n",
       "      <th>lot_size</th>\n",
       "      <th>basement</th>\n",
       "      <th>restaurants</th>\n",
       "      <th>groceries</th>\n",
       "      <th>nightlife</th>\n",
       "      <th>cafes</th>\n",
       "      <th>shopping</th>\n",
       "      <th>arts_entertainment</th>\n",
       "      <th>beauty_spas</th>\n",
       "      <th>active_life</th>\n",
       "      <th>median_age</th>\n",
       "      <th>married</th>\n",
       "      <th>college_grad</th>\n",
       "      <th>property_tax</th>\n",
       "      <th>insurance</th>\n",
       "      <th>median_school</th>\n",
       "      <th>num_schools</th>\n",
       "      <th>two_and_two</th>\n",
       "      <th>during_recession</th>\n",
       "      <th>property_age</th>\n",
       "      <th>school_score</th>\n",
       "      <th>exterior_walls_Brick</th>\n",
       "      <th>exterior_walls_Brick veneer</th>\n",
       "      <th>exterior_walls_Combination</th>\n",
       "      <th>exterior_walls_Metal</th>\n",
       "      <th>exterior_walls_Missing</th>\n",
       "      <th>exterior_walls_Other</th>\n",
       "      <th>exterior_walls_Siding (Alum/Vinyl)</th>\n",
       "      <th>exterior_walls_Wood</th>\n",
       "      <th>roof_Asphalt</th>\n",
       "      <th>roof_Composition Shingle</th>\n",
       "      <th>roof_Composition Shingles</th>\n",
       "      <th>roof_Missing</th>\n",
       "      <th>roof_Other</th>\n",
       "      <th>roof_Shake Shingle</th>\n",
       "      <th>property_type_Apartment / Condo / Townhouse</th>\n",
       "      <th>property_type_Single-Family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295850</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>584</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>107</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>89</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "      <td>58</td>\n",
       "      <td>33.000</td>\n",
       "      <td>65.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>234.000</td>\n",
       "      <td>81.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>612</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>105</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>39.000</td>\n",
       "      <td>73.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>169.000</td>\n",
       "      <td>51.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>9.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>279900</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>615</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>183</td>\n",
       "      <td>13</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>101</td>\n",
       "      <td>10</td>\n",
       "      <td>74</td>\n",
       "      <td>62</td>\n",
       "      <td>28.000</td>\n",
       "      <td>15.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>216.000</td>\n",
       "      <td>74.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>24.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>379900</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>618</td>\n",
       "      <td>33541</td>\n",
       "      <td>0.000</td>\n",
       "      <td>198</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>25</td>\n",
       "      <td>127</td>\n",
       "      <td>11</td>\n",
       "      <td>72</td>\n",
       "      <td>83</td>\n",
       "      <td>36.000</td>\n",
       "      <td>25.000</td>\n",
       "      <td>91.000</td>\n",
       "      <td>265.000</td>\n",
       "      <td>92.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>27.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>340000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>634</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>149</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>83</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>73</td>\n",
       "      <td>37.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>75.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>27.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tx_price  beds  baths  sqft  lot_size  basement  restaurants  groceries  \\\n",
       "0    295850     1      1   584         0     0.000          107          9   \n",
       "1    216500     1      1   612         0     1.000          105         15   \n",
       "2    279900     1      1   615         0     0.000          183         13   \n",
       "3    379900     1      1   618     33541     0.000          198          9   \n",
       "4    340000     1      1   634         0     0.000          149          7   \n",
       "\n",
       "   nightlife  cafes  shopping  arts_entertainment  beauty_spas  active_life  \\\n",
       "0         30     19        89                   6           47           58   \n",
       "1          6     13        87                   2           26           14   \n",
       "2         31     30       101                  10           74           62   \n",
       "3         38     25       127                  11           72           83   \n",
       "4         22     20        83                  10           50           73   \n",
       "\n",
       "   median_age  married  college_grad  property_tax  insurance  median_school  \\\n",
       "0      33.000   65.000        84.000       234.000     81.000          9.000   \n",
       "1      39.000   73.000        69.000       169.000     51.000          3.000   \n",
       "2      28.000   15.000        86.000       216.000     74.000          8.000   \n",
       "3      36.000   25.000        91.000       265.000     92.000          9.000   \n",
       "4      37.000   20.000        75.000        88.000     30.000          9.000   \n",
       "\n",
       "   num_schools  two_and_two  during_recession  property_age  school_score  \\\n",
       "0        3.000            0                 1             0        27.000   \n",
       "1        3.000            0                 0            41         9.000   \n",
       "2        3.000            0                 1            49        24.000   \n",
       "3        3.000            0                 0             5        27.000   \n",
       "4        3.000            0                 0            10        27.000   \n",
       "\n",
       "   exterior_walls_Brick  exterior_walls_Brick veneer  \\\n",
       "0                     0                            0   \n",
       "1                     1                            0   \n",
       "2                     0                            0   \n",
       "3                     0                            0   \n",
       "4                     1                            0   \n",
       "\n",
       "   exterior_walls_Combination  exterior_walls_Metal  exterior_walls_Missing  \\\n",
       "0                           0                     0                       0   \n",
       "1                           0                     0                       0   \n",
       "2                           0                     0                       0   \n",
       "3                           0                     0                       0   \n",
       "4                           0                     0                       0   \n",
       "\n",
       "   exterior_walls_Other  exterior_walls_Siding (Alum/Vinyl)  \\\n",
       "0                     0                                   0   \n",
       "1                     0                                   0   \n",
       "2                     0                                   0   \n",
       "3                     0                                   0   \n",
       "4                     0                                   0   \n",
       "\n",
       "   exterior_walls_Wood  roof_Asphalt  roof_Composition Shingle  \\\n",
       "0                    1             0                         0   \n",
       "1                    0             0                         1   \n",
       "2                    1             0                         0   \n",
       "3                    1             0                         0   \n",
       "4                    0             0                         0   \n",
       "\n",
       "   roof_Composition Shingles  roof_Missing  roof_Other  roof_Shake Shingle  \\\n",
       "0                          0             1           0                   0   \n",
       "1                          0             0           0                   0   \n",
       "2                          0             1           0                   0   \n",
       "3                          0             1           0                   0   \n",
       "4                          0             1           0                   0   \n",
       "\n",
       "   property_type_Apartment / Condo / Townhouse  property_type_Single-Family  \n",
       "0                                            1                            0  \n",
       "1                                            1                            0  \n",
       "2                                            1                            0  \n",
       "3                                            1                            0  \n",
       "4                                            1                            0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9iQX7_SaRhP"
   },
   "source": [
    "<br id=\"split\">\n",
    "\n",
    "# 1. Split your dataset\n",
    "\n",
    "Let's start with a crucial but sometimes overlooked step: **Splitting** your data.\n",
    "\n",
    "<br>\n",
    "First, let's import the <code style=\"color:steelblue\">train_test_split()</code> function from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6sKuL6cCaRhR"
   },
   "outputs": [],
   "source": [
    "# Function for splitting training and test set\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfmvemzkaRhU"
   },
   "source": [
    "Next, separate your dataframe into separate objects for the target variable (<code style=\"color:steelblue\">y</code>) and the input features (<code style=\"color:steelblue\">X</code>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bArrr7_0aRhW"
   },
   "outputs": [],
   "source": [
    "# Create separate object for target variable\n",
    "y = df.tx_price\n",
    "# Create separate object for input features\n",
    "X = df.drop('tx_price', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6bOn7vmaRhc"
   },
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "## <span style=\"color:RoyalBlue\">Exercise 5.1</span>\n",
    "\n",
    "**First, split <code style=\"color:steelblue\">X</code> and <code style=\"color:steelblue\">y</code> into training and test sets using the <code style=\"color:steelblue\">train_test_split()</code> function.** \n",
    "* **Tip:** Its first two arguments should be X and y.\n",
    "* **Pass in the argument <code style=\"color:steelblue\">test_size=<span style=\"color:crimson\">0.2</span></code> to set aside 20% of our observations for the test set.**\n",
    "* **Pass in <code style=\"color:steelblue\">random_state=<span style=\"color:crimson\">1234</span></code> to set the random state for replicable results.**\n",
    "* You can read more about this function in the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\" target=\"_blank\">documentation</a>.\n",
    "\n",
    "The function returns a tuple with 4 elements: <code style=\"color:steelblue\">(X_train, X_test, y_train, y_test)</code>. Remember, you can **unpack** it and save it into 4 seperate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZLACnEqaRhe"
   },
   "outputs": [],
   "source": [
    "# Split X and y into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBLcEszcaRh4"
   },
   "source": [
    "Let's confirm we have the right number of observations in each subset.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Next, run this code to confirm the size of each subset is correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAFcnGZAaRh5",
    "outputId": "8c264aa4-283f-4ede-eaa2-fb58648cb711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "X_train: (1490, 40)\n",
      "y_train (1490,)\n",
      "\n",
      "Testing Set:\n",
      "X_test: (373, 40)\n",
      "y_test (373,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set:\") \n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print()\n",
    "print(\"Testing Set:\")\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LwYTbb6OaRiD"
   },
   "source": [
    "Next, when we train our models, we can fit them on the <code style=\"color:steelblue\">X_train</code> feature values and <code style=\"color:steelblue\">y_train</code> target values.\n",
    "\n",
    "Finally, when we're ready to evaluate our models on our test set, we would use the trained models to predict <code style=\"color:steelblue\">X_test</code> and evaluate the predictions against <code style=\"color:steelblue\">y_test</code>.\n",
    "\n",
    "<hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "<div style=\"text-align:center; margin: 40px 0 40px 0;\">\n",
    "    \n",
    "[**Back to Contents**](#toc)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-LcQ5QNXaRiF"
   },
   "source": [
    "<br id=\"pipelines\">\n",
    "\n",
    "# 2. Build model pipelines\n",
    "\n",
    "In lesson 1, 2, and 3, you explored the dataset, cleaned it, and engineered new features. However, sometimes we'll want to preprocess the training data even more before feeding it into our algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-LcQ5QNXaRiF"
   },
   "source": [
    "<br>\n",
    "\n",
    "### Standardization\n",
    "First, let's show the summary statistics from our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "os-QZw6NaRiG",
    "outputId": "5f95c714-c8ac-4857-e42b-40d9c8cc66b4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beds</th>\n",
       "      <th>baths</th>\n",
       "      <th>sqft</th>\n",
       "      <th>lot_size</th>\n",
       "      <th>basement</th>\n",
       "      <th>restaurants</th>\n",
       "      <th>groceries</th>\n",
       "      <th>nightlife</th>\n",
       "      <th>cafes</th>\n",
       "      <th>shopping</th>\n",
       "      <th>arts_entertainment</th>\n",
       "      <th>beauty_spas</th>\n",
       "      <th>active_life</th>\n",
       "      <th>median_age</th>\n",
       "      <th>married</th>\n",
       "      <th>college_grad</th>\n",
       "      <th>property_tax</th>\n",
       "      <th>insurance</th>\n",
       "      <th>median_school</th>\n",
       "      <th>num_schools</th>\n",
       "      <th>two_and_two</th>\n",
       "      <th>during_recession</th>\n",
       "      <th>property_age</th>\n",
       "      <th>school_score</th>\n",
       "      <th>exterior_walls_Brick</th>\n",
       "      <th>exterior_walls_Brick veneer</th>\n",
       "      <th>exterior_walls_Combination</th>\n",
       "      <th>exterior_walls_Metal</th>\n",
       "      <th>exterior_walls_Missing</th>\n",
       "      <th>exterior_walls_Other</th>\n",
       "      <th>exterior_walls_Siding (Alum/Vinyl)</th>\n",
       "      <th>exterior_walls_Wood</th>\n",
       "      <th>roof_Asphalt</th>\n",
       "      <th>roof_Composition Shingle</th>\n",
       "      <th>roof_Composition Shingles</th>\n",
       "      <th>roof_Missing</th>\n",
       "      <th>roof_Other</th>\n",
       "      <th>roof_Shake Shingle</th>\n",
       "      <th>property_type_Apartment / Condo / Townhouse</th>\n",
       "      <th>property_type_Single-Family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.434</td>\n",
       "      <td>2.579</td>\n",
       "      <td>2322.785</td>\n",
       "      <td>12746.660</td>\n",
       "      <td>0.879</td>\n",
       "      <td>39.496</td>\n",
       "      <td>4.389</td>\n",
       "      <td>5.005</td>\n",
       "      <td>5.186</td>\n",
       "      <td>39.561</td>\n",
       "      <td>3.362</td>\n",
       "      <td>22.909</td>\n",
       "      <td>15.770</td>\n",
       "      <td>38.509</td>\n",
       "      <td>69.471</td>\n",
       "      <td>65.013</td>\n",
       "      <td>464.266</td>\n",
       "      <td>139.610</td>\n",
       "      <td>6.510</td>\n",
       "      <td>2.779</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.266</td>\n",
       "      <td>24.344</td>\n",
       "      <td>17.940</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.419</td>\n",
       "      <td>0.581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.073</td>\n",
       "      <td>0.930</td>\n",
       "      <td>1297.102</td>\n",
       "      <td>34805.545</td>\n",
       "      <td>0.327</td>\n",
       "      <td>46.986</td>\n",
       "      <td>4.498</td>\n",
       "      <td>8.442</td>\n",
       "      <td>7.443</td>\n",
       "      <td>52.335</td>\n",
       "      <td>4.694</td>\n",
       "      <td>25.724</td>\n",
       "      <td>17.999</td>\n",
       "      <td>6.615</td>\n",
       "      <td>19.865</td>\n",
       "      <td>17.093</td>\n",
       "      <td>227.250</td>\n",
       "      <td>71.511</td>\n",
       "      <td>1.975</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.442</td>\n",
       "      <td>21.209</td>\n",
       "      <td>6.452</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>500.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1351.000</td>\n",
       "      <td>1542.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>33.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>53.250</td>\n",
       "      <td>321.000</td>\n",
       "      <td>94.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1913.500</td>\n",
       "      <td>6183.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>15.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>38.000</td>\n",
       "      <td>74.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>426.000</td>\n",
       "      <td>125.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>18.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3014.750</td>\n",
       "      <td>11761.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>56.000</td>\n",
       "      <td>7.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>35.000</td>\n",
       "      <td>21.000</td>\n",
       "      <td>43.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>78.000</td>\n",
       "      <td>572.000</td>\n",
       "      <td>169.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>38.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>7842.000</td>\n",
       "      <td>436471.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>266.000</td>\n",
       "      <td>24.000</td>\n",
       "      <td>53.000</td>\n",
       "      <td>47.000</td>\n",
       "      <td>340.000</td>\n",
       "      <td>35.000</td>\n",
       "      <td>177.000</td>\n",
       "      <td>94.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>4508.000</td>\n",
       "      <td>1374.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>114.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          beds    baths     sqft   lot_size  basement  restaurants  groceries  \\\n",
       "count 1490.000 1490.000 1490.000   1490.000  1490.000     1490.000   1490.000   \n",
       "mean     3.434    2.579 2322.785  12746.660     0.879       39.496      4.389   \n",
       "std      1.073    0.930 1297.102  34805.545     0.327       46.986      4.498   \n",
       "min      1.000    1.000  500.000      0.000     0.000        0.000      0.000   \n",
       "25%      3.000    2.000 1351.000   1542.000     1.000        6.000      1.000   \n",
       "50%      4.000    3.000 1913.500   6183.000     1.000       21.000      3.000   \n",
       "75%      4.000    3.000 3014.750  11761.000     1.000       56.000      7.000   \n",
       "max      5.000    6.000 7842.000 436471.000     1.000      266.000     24.000   \n",
       "\n",
       "       nightlife    cafes  shopping  arts_entertainment  beauty_spas  \\\n",
       "count   1490.000 1490.000  1490.000            1490.000     1490.000   \n",
       "mean       5.005    5.186    39.561               3.362       22.909   \n",
       "std        8.442    7.443    52.335               4.694       25.724   \n",
       "min        0.000    0.000     0.000               0.000        0.000   \n",
       "25%        0.000    0.000     6.000               0.000        4.000   \n",
       "50%        2.000    3.000    20.000               2.000       15.000   \n",
       "75%        6.000    6.000    50.000               5.000       35.000   \n",
       "max       53.000   47.000   340.000              35.000      177.000   \n",
       "\n",
       "       active_life  median_age  married  college_grad  property_tax  \\\n",
       "count     1490.000    1490.000 1490.000      1490.000      1490.000   \n",
       "mean        15.770      38.509   69.471        65.013       464.266   \n",
       "std         17.999       6.615   19.865        17.093       227.250   \n",
       "min          0.000      22.000   11.000         5.000        88.000   \n",
       "25%          4.000      33.000   59.000        53.250       321.000   \n",
       "50%         10.000      38.000   74.000        66.000       426.000   \n",
       "75%         21.000      43.000   84.000        78.000       572.000   \n",
       "max         94.000      69.000  100.000       100.000      4508.000   \n",
       "\n",
       "       insurance  median_school  num_schools  two_and_two  during_recession  \\\n",
       "count   1490.000       1490.000     1490.000     1490.000          1490.000   \n",
       "mean     139.610          6.510        2.779        0.093             0.266   \n",
       "std       71.511          1.975        0.517        0.290             0.442   \n",
       "min       30.000          1.000        1.000        0.000             0.000   \n",
       "25%       94.000          5.000        3.000        0.000             0.000   \n",
       "50%      125.000          7.000        3.000        0.000             0.000   \n",
       "75%      169.000          8.000        3.000        0.000             1.000   \n",
       "max     1374.000         10.000        4.000        1.000             1.000   \n",
       "\n",
       "       property_age  school_score  exterior_walls_Brick  \\\n",
       "count      1490.000      1490.000              1490.000   \n",
       "mean         24.344        17.940                 0.360   \n",
       "std          21.209         6.452                 0.480   \n",
       "min           0.000         3.000                 0.000   \n",
       "25%           6.000        12.000                 0.000   \n",
       "50%          20.000        18.000                 0.000   \n",
       "75%          38.000        24.000                 1.000   \n",
       "max         114.000        30.000                 1.000   \n",
       "\n",
       "       exterior_walls_Brick veneer  exterior_walls_Combination  \\\n",
       "count                     1490.000                    1490.000   \n",
       "mean                         0.024                       0.059   \n",
       "std                          0.154                       0.236   \n",
       "min                          0.000                       0.000   \n",
       "25%                          0.000                       0.000   \n",
       "50%                          0.000                       0.000   \n",
       "75%                          0.000                       0.000   \n",
       "max                          1.000                       1.000   \n",
       "\n",
       "       exterior_walls_Metal  exterior_walls_Missing  exterior_walls_Other  \\\n",
       "count              1490.000                1490.000              1490.000   \n",
       "mean                  0.066                   0.119                 0.038   \n",
       "std                   0.248                   0.324                 0.190   \n",
       "min                   0.000                   0.000                 0.000   \n",
       "25%                   0.000                   0.000                 0.000   \n",
       "50%                   0.000                   0.000                 0.000   \n",
       "75%                   0.000                   0.000                 0.000   \n",
       "max                   1.000                   1.000                 1.000   \n",
       "\n",
       "       exterior_walls_Siding (Alum/Vinyl)  exterior_walls_Wood  roof_Asphalt  \\\n",
       "count                            1490.000             1490.000      1490.000   \n",
       "mean                                0.268                0.066         0.073   \n",
       "std                                 0.443                0.248         0.260   \n",
       "min                                 0.000                0.000         0.000   \n",
       "25%                                 0.000                0.000         0.000   \n",
       "50%                                 0.000                0.000         0.000   \n",
       "75%                                 1.000                0.000         0.000   \n",
       "max                                 1.000                1.000         1.000   \n",
       "\n",
       "       roof_Composition Shingle  roof_Composition Shingles  roof_Missing  \\\n",
       "count                  1490.000                   1490.000      1490.000   \n",
       "mean                      0.624                      0.019         0.189   \n",
       "std                       0.485                      0.138         0.392   \n",
       "min                       0.000                      0.000         0.000   \n",
       "25%                       0.000                      0.000         0.000   \n",
       "50%                       1.000                      0.000         0.000   \n",
       "75%                       1.000                      0.000         0.000   \n",
       "max                       1.000                      1.000         1.000   \n",
       "\n",
       "       roof_Other  roof_Shake Shingle  \\\n",
       "count    1490.000            1490.000   \n",
       "mean        0.060               0.034   \n",
       "std         0.238               0.180   \n",
       "min         0.000               0.000   \n",
       "25%         0.000               0.000   \n",
       "50%         0.000               0.000   \n",
       "75%         0.000               0.000   \n",
       "max         1.000               1.000   \n",
       "\n",
       "       property_type_Apartment / Condo / Townhouse  \\\n",
       "count                                     1490.000   \n",
       "mean                                         0.419   \n",
       "std                                          0.494   \n",
       "min                                          0.000   \n",
       "25%                                          0.000   \n",
       "50%                                          0.000   \n",
       "75%                                          1.000   \n",
       "max                                          1.000   \n",
       "\n",
       "       property_type_Single-Family  \n",
       "count                     1490.000  \n",
       "mean                         0.581  \n",
       "std                          0.494  \n",
       "min                          0.000  \n",
       "25%                          0.000  \n",
       "50%                          1.000  \n",
       "75%                          1.000  \n",
       "max                          1.000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics of X_train\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2RaxNGDcaRiL"
   },
   "source": [
    "Next, standardize the training data manually, creating a new <code style=\"color:steelblue\">X_train_new</code> object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AP4CIjMoaRiN"
   },
   "outputs": [],
   "source": [
    "# Standardize X_train\n",
    "X_train_new = (X_train - X_train.mean()) / X_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jx0l28rWaRiQ"
   },
   "source": [
    "Let's look at the summary statistics for <code style=\"color:steelblue\">X_train_new</code> to confirm standarization worked correctly.\n",
    "* How can you tell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AK4Kt8zfaRiR",
    "outputId": "e30cc5f5-da31-48e0-a9c9-70c800ec3a11"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beds</th>\n",
       "      <th>baths</th>\n",
       "      <th>sqft</th>\n",
       "      <th>lot_size</th>\n",
       "      <th>basement</th>\n",
       "      <th>restaurants</th>\n",
       "      <th>groceries</th>\n",
       "      <th>nightlife</th>\n",
       "      <th>cafes</th>\n",
       "      <th>shopping</th>\n",
       "      <th>arts_entertainment</th>\n",
       "      <th>beauty_spas</th>\n",
       "      <th>active_life</th>\n",
       "      <th>median_age</th>\n",
       "      <th>married</th>\n",
       "      <th>college_grad</th>\n",
       "      <th>property_tax</th>\n",
       "      <th>insurance</th>\n",
       "      <th>median_school</th>\n",
       "      <th>num_schools</th>\n",
       "      <th>two_and_two</th>\n",
       "      <th>during_recession</th>\n",
       "      <th>property_age</th>\n",
       "      <th>school_score</th>\n",
       "      <th>exterior_walls_Brick</th>\n",
       "      <th>exterior_walls_Brick veneer</th>\n",
       "      <th>exterior_walls_Combination</th>\n",
       "      <th>exterior_walls_Metal</th>\n",
       "      <th>exterior_walls_Missing</th>\n",
       "      <th>exterior_walls_Other</th>\n",
       "      <th>exterior_walls_Siding (Alum/Vinyl)</th>\n",
       "      <th>exterior_walls_Wood</th>\n",
       "      <th>roof_Asphalt</th>\n",
       "      <th>roof_Composition Shingle</th>\n",
       "      <th>roof_Composition Shingles</th>\n",
       "      <th>roof_Missing</th>\n",
       "      <th>roof_Other</th>\n",
       "      <th>roof_Shake Shingle</th>\n",
       "      <th>property_type_Apartment / Condo / Townhouse</th>\n",
       "      <th>property_type_Single-Family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "      <td>1490.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.269</td>\n",
       "      <td>-1.697</td>\n",
       "      <td>-1.405</td>\n",
       "      <td>-0.366</td>\n",
       "      <td>-2.688</td>\n",
       "      <td>-0.841</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>-0.697</td>\n",
       "      <td>-0.756</td>\n",
       "      <td>-0.716</td>\n",
       "      <td>-0.891</td>\n",
       "      <td>-0.876</td>\n",
       "      <td>-2.496</td>\n",
       "      <td>-2.943</td>\n",
       "      <td>-3.511</td>\n",
       "      <td>-1.656</td>\n",
       "      <td>-1.533</td>\n",
       "      <td>-2.790</td>\n",
       "      <td>-3.440</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>-1.148</td>\n",
       "      <td>-2.316</td>\n",
       "      <td>-0.749</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.606</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-1.288</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-1.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.405</td>\n",
       "      <td>-0.622</td>\n",
       "      <td>-0.749</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-0.713</td>\n",
       "      <td>-0.753</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>-0.697</td>\n",
       "      <td>-0.641</td>\n",
       "      <td>-0.716</td>\n",
       "      <td>-0.735</td>\n",
       "      <td>-0.654</td>\n",
       "      <td>-0.833</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>-0.630</td>\n",
       "      <td>-0.638</td>\n",
       "      <td>-0.765</td>\n",
       "      <td>0.427</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>-0.865</td>\n",
       "      <td>-0.921</td>\n",
       "      <td>-0.749</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.606</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>-1.288</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>-1.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.527</td>\n",
       "      <td>0.452</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>0.372</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.356</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.374</td>\n",
       "      <td>-0.290</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>-0.321</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.427</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.749</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.606</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>0.776</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.527</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.533</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.427</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>1.662</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.939</td>\n",
       "      <td>1.334</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>1.650</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>-0.281</td>\n",
       "      <td>0.776</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.483</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>1.176</td>\n",
       "      <td>0.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.459</td>\n",
       "      <td>3.676</td>\n",
       "      <td>4.255</td>\n",
       "      <td>12.174</td>\n",
       "      <td>0.372</td>\n",
       "      <td>4.821</td>\n",
       "      <td>4.360</td>\n",
       "      <td>5.685</td>\n",
       "      <td>5.618</td>\n",
       "      <td>5.741</td>\n",
       "      <td>6.741</td>\n",
       "      <td>5.990</td>\n",
       "      <td>4.346</td>\n",
       "      <td>4.609</td>\n",
       "      <td>1.537</td>\n",
       "      <td>2.047</td>\n",
       "      <td>17.794</td>\n",
       "      <td>17.262</td>\n",
       "      <td>1.767</td>\n",
       "      <td>2.360</td>\n",
       "      <td>3.129</td>\n",
       "      <td>1.662</td>\n",
       "      <td>4.227</td>\n",
       "      <td>1.869</td>\n",
       "      <td>1.334</td>\n",
       "      <td>6.353</td>\n",
       "      <td>3.990</td>\n",
       "      <td>3.768</td>\n",
       "      <td>2.714</td>\n",
       "      <td>5.059</td>\n",
       "      <td>1.650</td>\n",
       "      <td>3.768</td>\n",
       "      <td>3.558</td>\n",
       "      <td>0.776</td>\n",
       "      <td>7.095</td>\n",
       "      <td>2.069</td>\n",
       "      <td>3.943</td>\n",
       "      <td>5.365</td>\n",
       "      <td>1.176</td>\n",
       "      <td>0.850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          beds    baths     sqft  lot_size  basement  restaurants  groceries  \\\n",
       "count 1490.000 1490.000 1490.000  1490.000  1490.000     1490.000   1490.000   \n",
       "mean    -0.000   -0.000    0.000     0.000     0.000        0.000      0.000   \n",
       "std      1.000    1.000    1.000     1.000     1.000        1.000      1.000   \n",
       "min     -2.269   -1.697   -1.405    -0.366    -2.688       -0.841     -0.976   \n",
       "25%     -0.405   -0.622   -0.749    -0.322     0.372       -0.713     -0.753   \n",
       "50%      0.527    0.452   -0.316    -0.189     0.372       -0.394     -0.309   \n",
       "75%      0.527    0.452    0.533    -0.028     0.372        0.351      0.581   \n",
       "max      1.459    3.676    4.255    12.174     0.372        4.821      4.360   \n",
       "\n",
       "       nightlife    cafes  shopping  arts_entertainment  beauty_spas  \\\n",
       "count   1490.000 1490.000  1490.000            1490.000     1490.000   \n",
       "mean       0.000    0.000     0.000              -0.000        0.000   \n",
       "std        1.000    1.000     1.000               1.000        1.000   \n",
       "min       -0.593   -0.697    -0.756              -0.716       -0.891   \n",
       "25%       -0.593   -0.697    -0.641              -0.716       -0.735   \n",
       "50%       -0.356   -0.294    -0.374              -0.290       -0.307   \n",
       "75%        0.118    0.109     0.199               0.349        0.470   \n",
       "max        5.685    5.618     5.741               6.741        5.990   \n",
       "\n",
       "       active_life  median_age  married  college_grad  property_tax  \\\n",
       "count     1490.000    1490.000 1490.000      1490.000      1490.000   \n",
       "mean         0.000      -0.000   -0.000        -0.000         0.000   \n",
       "std          1.000       1.000    1.000         1.000         1.000   \n",
       "min         -0.876      -2.496   -2.943        -3.511        -1.656   \n",
       "25%         -0.654      -0.833   -0.527        -0.688        -0.630   \n",
       "50%         -0.321      -0.077    0.228         0.058        -0.168   \n",
       "75%          0.291       0.679    0.731         0.760         0.474   \n",
       "max          4.346       4.609    1.537         2.047        17.794   \n",
       "\n",
       "       insurance  median_school  num_schools  two_and_two  during_recession  \\\n",
       "count   1490.000       1490.000     1490.000     1490.000          1490.000   \n",
       "mean      -0.000          0.000        0.000       -0.000            -0.000   \n",
       "std        1.000          1.000        1.000        1.000             1.000   \n",
       "min       -1.533         -2.790       -3.440       -0.319            -0.601   \n",
       "25%       -0.638         -0.765        0.427       -0.319            -0.601   \n",
       "50%       -0.204          0.248        0.427       -0.319            -0.601   \n",
       "75%        0.411          0.754        0.427       -0.319             1.662   \n",
       "max       17.262          1.767        2.360        3.129             1.662   \n",
       "\n",
       "       property_age  school_score  exterior_walls_Brick  \\\n",
       "count      1490.000      1490.000              1490.000   \n",
       "mean         -0.000        -0.000                 0.000   \n",
       "std           1.000         1.000                 1.000   \n",
       "min          -1.148        -2.316                -0.749   \n",
       "25%          -0.865        -0.921                -0.749   \n",
       "50%          -0.205         0.009                -0.749   \n",
       "75%           0.644         0.939                 1.334   \n",
       "max           4.227         1.869                 1.334   \n",
       "\n",
       "       exterior_walls_Brick veneer  exterior_walls_Combination  \\\n",
       "count                     1490.000                    1490.000   \n",
       "mean                         0.000                       0.000   \n",
       "std                          1.000                       1.000   \n",
       "min                         -0.157                      -0.250   \n",
       "25%                         -0.157                      -0.250   \n",
       "50%                         -0.157                      -0.250   \n",
       "75%                         -0.157                      -0.250   \n",
       "max                          6.353                       3.990   \n",
       "\n",
       "       exterior_walls_Metal  exterior_walls_Missing  exterior_walls_Other  \\\n",
       "count              1490.000                1490.000              1490.000   \n",
       "mean                  0.000                   0.000                 0.000   \n",
       "std                   1.000                   1.000                 1.000   \n",
       "min                  -0.265                  -0.368                -0.198   \n",
       "25%                  -0.265                  -0.368                -0.198   \n",
       "50%                  -0.265                  -0.368                -0.198   \n",
       "75%                  -0.265                  -0.368                -0.198   \n",
       "max                   3.768                   2.714                 5.059   \n",
       "\n",
       "       exterior_walls_Siding (Alum/Vinyl)  exterior_walls_Wood  roof_Asphalt  \\\n",
       "count                            1490.000             1490.000      1490.000   \n",
       "mean                                0.000               -0.000        -0.000   \n",
       "std                                 1.000                1.000         1.000   \n",
       "min                                -0.606               -0.265        -0.281   \n",
       "25%                                -0.606               -0.265        -0.281   \n",
       "50%                                -0.606               -0.265        -0.281   \n",
       "75%                                 1.650               -0.265        -0.281   \n",
       "max                                 1.650                3.768         3.558   \n",
       "\n",
       "       roof_Composition Shingle  roof_Composition Shingles  roof_Missing  \\\n",
       "count                  1490.000                   1490.000      1490.000   \n",
       "mean                     -0.000                     -0.000         0.000   \n",
       "std                       1.000                      1.000         1.000   \n",
       "min                      -1.288                     -0.141        -0.483   \n",
       "25%                      -1.288                     -0.141        -0.483   \n",
       "50%                       0.776                     -0.141        -0.483   \n",
       "75%                       0.776                     -0.141        -0.483   \n",
       "max                       0.776                      7.095         2.069   \n",
       "\n",
       "       roof_Other  roof_Shake Shingle  \\\n",
       "count    1490.000            1490.000   \n",
       "mean       -0.000               0.000   \n",
       "std         1.000               1.000   \n",
       "min        -0.253              -0.186   \n",
       "25%        -0.253              -0.186   \n",
       "50%        -0.253              -0.186   \n",
       "75%        -0.253              -0.186   \n",
       "max         3.943               5.365   \n",
       "\n",
       "       property_type_Apartment / Condo / Townhouse  \\\n",
       "count                                     1490.000   \n",
       "mean                                         0.000   \n",
       "std                                          1.000   \n",
       "min                                         -0.850   \n",
       "25%                                         -0.850   \n",
       "50%                                         -0.850   \n",
       "75%                                          1.176   \n",
       "max                                          1.176   \n",
       "\n",
       "       property_type_Single-Family  \n",
       "count                     1490.000  \n",
       "mean                        -0.000  \n",
       "std                          1.000  \n",
       "min                         -1.176  \n",
       "25%                         -1.176  \n",
       "50%                          0.850  \n",
       "75%                          0.850  \n",
       "max                          0.850  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics of X_train_new\n",
    "X_train_new.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Pipleline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KnY3Nu58aRij"
   },
   "source": [
    "For the most part, we'll almost never perform manual standardization because we'll include preprocessing steps in **model pipelines**.\n",
    "\n",
    "<br>\n",
    "So let's import the <code style=\"color:steelblue\">make_pipeline()</code> function from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bnsoYYCuaRik"
   },
   "outputs": [],
   "source": [
    "# Function for creating model pipelines\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f1kdKe87aRin"
   },
   "source": [
    "Now let's import the <code style=\"color:steelblue\">StandardScaler</code>, which is used for standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FDIGBg5iaRip"
   },
   "outputs": [],
   "source": [
    "# For standardization\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-Jim2ppaRi1"
   },
   "source": [
    "### Next, create a <code style=\"color:steelblue\">pipelines</code> dictionary.\n",
    "\n",
    "* It should include 3 keys: <code style=\"color:crimson\">'lasso'</code>, <code style=\"color:crimson\">'ridge'</code>, and <code style=\"color:crimson\">'enet'</code>\n",
    "* The corresponding values should be pipelines that first standardize the data.\n",
    "* For the algorithm in each pipeline, set <code style=\"color:steelblue\">random_state=<span style=\"color:crimson\">123</span></code> to ensure replicable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9X3N97MJaRi1"
   },
   "outputs": [],
   "source": [
    "# Create pipelines dictionary\n",
    "pipeline_dict = { 'lasso' : make_pipeline(StandardScaler(), Lasso(random_state=123)),\n",
    "                 'ridge' : make_pipeline(StandardScaler(), Ridge(random_state=123)),\n",
    "                 'enet' : make_pipeline(StandardScaler(), ElasticNet(random_state=123)) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vFZHWwTdaRi5"
   },
   "source": [
    "In the next exercise, you'll add pipelines for tree ensembles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S424C06paRi7"
   },
   "source": [
    "<hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "\n",
    "## <span style=\"color:RoyalBlue\">Exercise 5.2</span>\n",
    "\n",
    "**Add pipelines for <code style=\"color:SteelBlue\">RandomForestRegressor</code> and <code style=\"color:SteelBlue\">GradientBoostingRegressor</code> to your pipeline dictionary.**\n",
    "* Name them <code style=\"color:crimson\">'rf'</code> for random forest and <code style=\"color:crimson\">'gb'</code> for gradient boosted tree.\n",
    "* Both pipelines should standardize the data first.\n",
    "* For both, set <code style=\"color:steelblue\">random_state=<span style=\"color:crimson\">123</span></code> to ensure replicable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uerZJ228aRi8"
   },
   "outputs": [],
   "source": [
    "# Add a pipeline for 'rf' to 'pipeline_dict'\n",
    "pipeline_dict['rf'] = make_pipeline(StandardScaler(), RandomForestRegressor(random_state=123))\n",
    "\n",
    "# Add a pipeline for 'gb' to 'pipeline_dict'\n",
    "pipeline_dict['gb'] = make_pipeline(StandardScaler(), GradientBoostingRegressor(random_state=123))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PHId09qfaRjA"
   },
   "source": [
    "Let's make sure our dictionary has pipelines for each of our algorithms.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Run this code to confirm that you have all 5 algorithms, each part of a pipeline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b3RUYB_laRjB",
    "outputId": "65d269a2-5ff2-44f9-f32f-42c59fe7ba2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso <class 'sklearn.pipeline.Pipeline'>\n",
      "ridge <class 'sklearn.pipeline.Pipeline'>\n",
      "enet <class 'sklearn.pipeline.Pipeline'>\n",
      "rf <class 'sklearn.pipeline.Pipeline'>\n",
      "gb <class 'sklearn.pipeline.Pipeline'>\n"
     ]
    }
   ],
   "source": [
    "# Check that we have all 5 algorithms, and that they are all pipelines\n",
    "for key, value in pipeline_dict.items():\n",
    "    print( key, type(value) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDkbw7W-aRjH"
   },
   "source": [
    "Now that we have our pipelines, we're ready to move on to declaring hyperparameters to tune.\n",
    "\n",
    "<hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "\n",
    "<div style=\"text-align:center; margin: 40px 0 40px 0;\">\n",
    "    \n",
    "[**Back to Contents**](#toc)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZrrXXxD4aRjI"
   },
   "source": [
    "<br id=\"hyperparameters\">\n",
    "\n",
    "# 3. Declare hyperparameters to tune\n",
    "\n",
    "Up to now, we've been casually talking about \"tuning\" models, but now it's time to treat the topic more formally.\n",
    "\n",
    "<br>\n",
    "\n",
    "**First, list all the tunable hyperparameters for your Lasso regression pipeline.** We can do this to any Scikit Learn algorithm —to see what hyperparameters can be tuned. This is much more of an art than a science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vaJHUO_DaRjX",
    "outputId": "ab693fc1-4db2-4e8a-958c-16903b2c9507"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('standardscaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('lasso', Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=123,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False))],\n",
       " 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'lasso': Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "    normalize=False, positive=False, precompute=False, random_state=123,\n",
       "    selection='cyclic', tol=0.0001, warm_start=False),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'lasso__alpha': 1.0,\n",
       " 'lasso__copy_X': True,\n",
       " 'lasso__fit_intercept': True,\n",
       " 'lasso__max_iter': 1000,\n",
       " 'lasso__normalize': False,\n",
       " 'lasso__positive': False,\n",
       " 'lasso__precompute': False,\n",
       " 'lasso__random_state': 123,\n",
       " 'lasso__selection': 'cyclic',\n",
       " 'lasso__tol': 0.0001,\n",
       " 'lasso__warm_start': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List tuneable hyperparameters of our Lasso pipeline\n",
    "pipeline_dict['lasso'].get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C2OVbhsFaRjd"
   },
   "source": [
    "Next, declare hyperparameters to tune for Lasso and Ridge regression.\n",
    "* Try values between 0.001 and 10 for <code style=\"color:steelblue\">alpha</code>.\n",
    "\n",
    "> **ex:** [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqYRxIBraRje"
   },
   "outputs": [],
   "source": [
    "# Lasso hyperparameters\n",
    "lasso_hyperparameters = { 'lasso__alpha' : [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10] }\n",
    "\n",
    "# Ridge hyperparameters \n",
    "ridge_hyperparameters = { 'ridge__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HBZipCLWaRji"
   },
   "source": [
    "Now declare a hyperparameter grid fo Elastic-Net.\n",
    "* You should tune the <code style=\"color:steelblue\">l1_ratio</code> in addition to <code style=\"color:steelblue\">alpha</code>.\n",
    "\n",
    "* Try values between 0.1 and 0.9 for <code style=\"color:steelblue\">l1_ratio</code>.\n",
    "> **ex:** [0.1, 0.3, 0.5, 0.7, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v3eIAEzhaRjj"
   },
   "outputs": [],
   "source": [
    "# Elastic Net hyperparameters\n",
    "enet_hyperparameters = {\n",
    "    'elasticnet__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10], \n",
    "    'elasticnet__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zG3wl9DQaRjm"
   },
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "## <span style=\"color:RoyalBlue\">Exercise 5.3</span>\n",
    "\n",
    "Let's start by declaring the hyperparameter grid for our random forest.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Declare a hyperparameter grid for <code style=\"color:SteelBlue\">RandomForestRegressor</code>.**\n",
    "* Name it <code style=\"color:steelblue\">rf_hyperparameters</code>\n",
    "\n",
    "* Set <code style=\"color:steelblue\"><span style=\"color:crimson\">'randomforestregressor__n_estimators'</span>: [100, 200]</code>\n",
    "* Set <code style=\"color:steelblue\"><span style=\"color:crimson\">'randomforestregressor__max_features'</span>: ['auto', 'sqrt', 0.33]</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2av54cNdaRjn"
   },
   "outputs": [],
   "source": [
    "# Random forest hyperparameters\n",
    "rf_hyperparameters = { \n",
    "    'randomforestregressor__n_estimators' : [100, 200],\n",
    "    'randomforestregressor__max_features': ['auto', 'sqrt', 0.33],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Yx31VNRaRjr"
   },
   "source": [
    "Next, let's declare settings to try for our boosted tree.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Declare a hyperparameter grid for <code style=\"color:SteelBlue\">GradientBoostingRegressor</code>.**\n",
    "* Name it <code style=\"color:steelblue\">gb_hyperparameters</code>.\n",
    "* Set <code style=\"color:steelblue\"><span style=\"color:crimson\">'gradientboostingregressor__n_estimators'</span>: [100, 200]</code>\n",
    "* Set <code style=\"color:steelblue\"><span style=\"color:crimson\">'gradientboostingregressor__learning_rate'</span>: [0.05, 0.1, 0.2]</code>\n",
    "* Set <code style=\"color:steelblue\"><span style=\"color:crimson\">'gradientboostingregressor__max_depth'</span>: [1, 3, 5]</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rkkSpS3CaRj4"
   },
   "outputs": [],
   "source": [
    "# Boosted tree hyperparameters\n",
    "gb_hyperparameters = { \n",
    "    'gradientboostingregressor__n_estimators': [100, 200],\n",
    "    'gradientboostingregressor__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'gradientboostingregressor__max_depth': [1, 3, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CgnzoMTGaRkQ"
   },
   "source": [
    "## Now that we have all of our hyperparameters declared, let's store them in a dictionary for ease of access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CgnzoMTGaRkQ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### Create a <code style=\"color:steelblue\">hyperparameters</code> dictionary.\n",
    "* Use the same keys as in the <code style=\"color:steelblue\">pipelines</code> dictionary.\n",
    "    * If you forgot what those keys were, you can insert a new code cell and call <code style=\"color:steelblue\">pipelines.keys()</code> for a reminder.\n",
    "* Set the values to the corresponding **hyperparameter grids** we've been declaring throughout this module.\n",
    "    * e.g. <code style=\"color:steelblue\"><span style=\"color:crimson\">'rf'</span> : rf_hyperparameters</code>\n",
    "    * e.g. <code style=\"color:steelblue\"><span style=\"color:crimson\">'lasso'</span> : lasso_hyperparameters</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MYroG5QkaRkR"
   },
   "outputs": [],
   "source": [
    "# Create hyperparameters dictionary\n",
    "hyperparameters = {\n",
    "    'rf' : rf_hyperparameters,\n",
    "    'gb' : gb_hyperparameters,\n",
    "    'lasso' : lasso_hyperparameters,\n",
    "    'ridge' : ridge_hyperparameters,\n",
    "    'enet' : enet_hyperparameters\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbgxsHQhaRkX"
   },
   "source": [
    "**Finally, run this code to check that <code style=\"color:steelblue\">hyperparameters</code> is set up correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e34KQVzEaRkY",
    "outputId": "f47476ce-d18b-4c6b-bbc4-5e2f4d24a31c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enet was found in hyperparameters, and it is a grid.\n",
      "gb was found in hyperparameters, and it is a grid.\n",
      "ridge was found in hyperparameters, and it is a grid.\n",
      "rf was found in hyperparameters, and it is a grid.\n",
      "lasso was found in hyperparameters, and it is a grid.\n"
     ]
    }
   ],
   "source": [
    "for key in ['enet', 'gb', 'ridge', 'rf', 'lasso']:\n",
    "    \n",
    "    if key in hyperparameters:\n",
    "        \n",
    "        if type(hyperparameters[key]) is dict:\n",
    "            print( key, 'was found in hyperparameters, and it is a grid.' )\n",
    "            \n",
    "        else:\n",
    "            print( key, 'was found in hyperparameters, but it is not a grid.' )\n",
    "            \n",
    "    else:\n",
    "        print( key, 'was not found in hyperparameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "smPdEnexaRkg"
   },
   "source": [
    "<hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "<div style=\"text-align:center; margin: 40px 0 40px 0;\">\n",
    "    \n",
    "[**Back to Contents**](#toc)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPaHm-eIaRkg"
   },
   "source": [
    "<br id=\"fit-tune\">\n",
    "\n",
    "# 4. Fit and tune models with cross-validation\n",
    "\n",
    "Now that we have our <code style=\"color:steelblue\">pipelines</code> and <code style=\"color:steelblue\">hyperparameters</code> dictionaries declared, we're ready to tune our models with cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPaHm-eIaRkg"
   },
   "source": [
    "### Cross-Validation on a Single Model\n",
    "First, let's to import a helper for cross-validation called <code style=\"color:steelblue\">GridSearchCV</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_KSp804aRki"
   },
   "outputs": [],
   "source": [
    "# Helper for cross-validation\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b8cZ42YsaRko"
   },
   "source": [
    "Next, to see an example, set up cross-validation for Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gbdys2pGaRkp"
   },
   "outputs": [],
   "source": [
    "# Create cross-validation object from Lasso pipeline and Lasso hyperparameters\n",
    "model = GridSearchCV(pipeline_dict['lasso'], hyperparameters['lasso'], cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uTy-mJiIaRku"
   },
   "source": [
    "Pass <code style=\"color:steelblue\">X_train</code> and <code style=\"color:steelblue\">y_train</code> into the <code style=\"color:steelblue\">.fit()</code> function to tune hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K__o6yjTaRku",
    "outputId": "696fbf4f-ff69-45a2-cf46-c0255308828e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lasso', Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=123,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'lasso__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and tune model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9utTUsmaRlS"
   },
   "source": [
    "By the way, don't worry if you get the message:\n",
    "\n",
    "<pre style=\"color:crimson\">ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations</pre>\n",
    "\n",
    "We'll dive into some of the under-the-hood nuances later.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9utTUsmaRlS"
   },
   "source": [
    "<br>\n",
    "\n",
    "### In the next exercise, we'll write a loop that tunes all of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqwQ3EqwaRlT"
   },
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "## <span style=\"color:RoyalBlue\">Exercise 5.4</span>\n",
    "\n",
    "**Create a dictionary of models named <code style=\"color:SteelBlue\">fitted_models</code> that have been tuned using cross-validation.**\n",
    "* The keys should be the same as those in the <code style=\"color:SteelBlue\">pipelines</code> and <code style=\"color:SteelBlue\">hyperparameters</code> dictionaries. \n",
    "* The values should be <code style=\"color:steelblue\">GridSearchCV</code> objects that have been fitted to <code style=\"color:steelblue\">X_train</code> and <code style=\"color:steelblue\">y_train</code>.\n",
    "* After fitting each model, print <code style=\"color:crimson\">'name, \"has been fitted.\"'</code> just to track the progress.\n",
    "\n",
    "This step can take a few minutes, so please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTGRCPAbaRlV",
    "outputId": "0930a987-899b-4b53-ad5a-d8b1befdea69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso has been fitted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge has been fitted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enet has been fitted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf has been fitted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb has been fitted.\n"
     ]
    }
   ],
   "source": [
    "# Create empty dictionary called fitted_models\n",
    "fitted_models = {}\n",
    "\n",
    "# Loop through pipeline_dict.items(), grabing the name and pipeline, creating a new model and tuning it on each iteration.\n",
    "for name, pipeline in pipeline_dict.items():\n",
    "    \n",
    "    # 1. Create cross-validation object from pipeline and hyperparameters\n",
    "    model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1)\n",
    "    \n",
    "    # 2. Fit model on X_train, y_train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 3. Store model in fitted_models[name] \n",
    "    fitted_models[name] = model\n",
    "    \n",
    "    # 4. Print name 'has been fitted'\n",
    "    print(name, 'has been fitted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-N3f10pGaRlZ"
   },
   "source": [
    "<br>\n",
    "\n",
    "**Run this code to check that the models are of the correct type.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "py9fwHgeaRla",
    "outputId": "edd5ee8b-5bce-4643-dcf2-6a66d35b599a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso <class 'sklearn.model_selection._search.GridSearchCV'>\n",
      "ridge <class 'sklearn.model_selection._search.GridSearchCV'>\n",
      "enet <class 'sklearn.model_selection._search.GridSearchCV'>\n",
      "rf <class 'sklearn.model_selection._search.GridSearchCV'>\n",
      "gb <class 'sklearn.model_selection._search.GridSearchCV'>\n"
     ]
    }
   ],
   "source": [
    "# Check that we have 5 cross-validation objects\n",
    "for key, value in fitted_models.items():\n",
    "    print(key, type(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "--bBcinDaRle"
   },
   "source": [
    "<br>\n",
    "\n",
    "**Finally, run this code to check that the models have been fitted correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lpjEkfbHaRle",
    "outputId": "f2c9d3ed-4b8d-46de-8475-7ea21f8a137a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[436964.72454645 442963.9142652  291463.0170981  325223.16072052\n",
      " 433882.40681824 324235.68329556 365157.52441179 388464.28522182\n",
      " 319429.7044118  484743.6595536  345045.67584217 343483.41235351\n",
      " 351002.57171124 327692.71629481 302841.70058931 363883.65451154\n",
      " 448530.8737603  425908.83979536 441921.04238317 383539.44100754\n",
      " 383147.4394855  292788.46187974 360018.28402376 396718.84901236\n",
      " 303575.74867549 654366.59119543 399291.28127537 352474.12800986\n",
      " 331355.81744863 556753.3237877  547559.03374756 394799.74767358\n",
      " 374827.07387988 402828.48725302 421582.85063124 506450.48641716\n",
      " 256028.10694847 344472.44730711 328279.17425873 312917.245883\n",
      " 429006.26949103 439746.43069995 467402.9621547  244415.52648916\n",
      " 302943.3954336  739328.36655863 481814.24073093 589867.22204769\n",
      " 555732.98297512 394991.78547605 439842.2175305  450372.11528497\n",
      " 459911.79518048 365659.80762398 463469.38533881 378908.36329554\n",
      " 417137.98391151 399900.99393243 481219.67159236 414895.98952571\n",
      " 334076.6826531  466920.19889926 568275.40765945 370735.61388572\n",
      " 481729.36995964 416947.03003427 558337.83328816 703110.74676263\n",
      " 397026.52530352 405950.46565345 558151.21883341 453869.73955845\n",
      " 480635.51944939 354549.5991691  494104.5013102  438915.68941749\n",
      " 417153.51142952 429485.3944214  280929.34609974 446701.19127866\n",
      " 443726.62185449 380904.92795354 519864.53602661 332885.18831392\n",
      " 610776.6978702  276765.84221237 471280.03131187 274199.29503695\n",
      " 460903.32510349 383335.15146933 470963.16187077 667542.67006409\n",
      " 365404.3374487  425748.51766576 401946.42327618 388985.94003717\n",
      " 301192.50121285 417760.13083499 462123.19720829 471782.39655343\n",
      " 460427.88629789 339213.03368677 446674.11801832 330303.53436382\n",
      " 294698.12650787 290771.12717758 417261.53605989 553754.23628527\n",
      " 400279.33614628 401658.52884258 461258.66965324 387763.59483137\n",
      " 453333.66789785 405957.71381546 582085.8901357  541838.01413182\n",
      " 492927.97085464 387511.03254241 504562.43868204 369059.98975825\n",
      " 348951.91615346 482486.31211376 489679.32446731 360172.80357603\n",
      " 447781.69048452 358594.70202627 422507.07091391 301109.23422882\n",
      " 420356.44649425 467642.96907353 775215.15407696 387951.70382993\n",
      " 308623.78963871 355030.04700307 409595.54971104 326724.17288058\n",
      " 397628.46893665 465945.37888954 313575.58326031 525000.94549771\n",
      " 488077.80356378 267051.04125263 522923.36401096 445218.58305716\n",
      " 383169.09742087 339227.50528074 327282.65268721 357052.68641134\n",
      " 361296.1912375  309554.1427625  336759.1489142  377063.82116588\n",
      " 726992.06597281 613659.36961482 457178.2310098  395479.31051889\n",
      " 423549.2567469  499369.39159928 458719.35339668 416316.26466314\n",
      " 456542.87538445 415424.52954096 327168.08693531 427525.75821829\n",
      " 418926.56446728 497050.65895777 346322.91164546 398405.71357737\n",
      " 546449.94134432 316221.22847064 298571.76396841 596203.108589\n",
      " 323940.90635453 356151.06930276 506381.31306191 413959.60937606\n",
      " 378199.01853212 413095.18738029 386423.6401384  516309.67841526\n",
      " 411123.27456547 287463.55632264 444425.82854396 408319.41992205\n",
      " 544508.77659581 425141.83534403 320775.37117222 326281.52591146\n",
      " 580095.19242802 347649.93525068 506120.1378617  537384.30089495\n",
      " 426979.87886416 349361.61279183 392740.17165811 377025.02132912\n",
      " 363343.89770132 459882.10760837 536147.9649077  288622.1506507\n",
      " 415682.53333181 351620.89501798 387527.87624761 681631.69899203\n",
      " 548672.7139721  359794.10563576 518643.85493147 463366.21548595\n",
      " 327824.31551871 358020.03970855 334497.13788233 299640.0760169\n",
      " 559366.98174923 384229.75506175 408563.10616603 508546.53654176\n",
      " 432550.78684794 474724.3482372  380295.61430672 478312.39209918\n",
      " 365194.4594857  409303.04193465 387184.73914044 322960.05138026\n",
      " 342204.42372269 401488.76002011 447555.1388216  375995.42214254\n",
      " 496857.27808964 310721.36637329 374451.35783912 276828.88094551\n",
      " 383425.80853674 340516.2362432  423318.18798889 446250.05998625\n",
      " 382006.67026128 356802.62588603 358392.2339032  445165.89987884\n",
      " 280734.32703221 434789.92490151 705171.93774445 467014.94046723\n",
      " 301823.56190556 464552.80173379 383219.67152899 417544.01546798\n",
      " 512357.48237765 432005.24620269 330890.6390085  538851.15310648\n",
      " 392614.73329992 481217.03843886 407772.82579706 459595.44544912\n",
      " 409393.60615152 321252.83118631 354136.53753699 484321.40355524\n",
      " 343556.70554211 388349.34927608 498131.08819493 795877.25078757\n",
      " 476631.09259718 309014.77526342 529550.22498533 406353.40442933\n",
      " 476705.15238007 365842.24707995 343462.50325293 295944.84989269\n",
      " 325080.33696469 496684.45019151 401165.21153367 329479.13350332\n",
      " 469068.32900767 421411.7671992  425744.4510133  366355.5206863\n",
      " 291890.3380372  375727.77303545 761907.21872348 333878.40145464\n",
      " 342629.89614536 451894.90720431 325136.39738979 518846.05949884\n",
      " 294846.74697111 438005.34413722 379906.01032563 335771.00201928\n",
      " 512509.0062121  455131.90031956 554281.65712934 550860.9535334\n",
      " 390926.52424366 269183.25002173 344994.24080757 328750.87487733\n",
      " 518578.18084122 316767.27920493 598120.86818254 512819.68553261\n",
      " 357538.01343806 440403.92935793 378686.30073839 354882.17919715\n",
      " 436173.64202528 354384.42018338 503004.53427331 346794.90304623\n",
      " 387105.36940238 401480.70525121 338675.0363596  461283.94685361\n",
      " 391298.17680911 387482.10867326 499412.29765395 361538.4739205\n",
      " 475286.76640543 294341.96968514 443461.53872562 352412.04883098\n",
      " 485877.63360169 445537.08178836 407484.14282397 415615.40979155\n",
      " 369330.43693132 437581.42699383 335200.60137024 325884.07753519\n",
      " 485201.54125694 612324.18435845 340262.3481056  546844.19004038\n",
      " 373404.15466665 421475.06599097 403890.05731267 280550.55599107\n",
      " 539603.67652221 441325.59432166 486839.91700994 403938.89229201\n",
      " 483166.54855599 332553.17809371 443365.90761512 415150.97272582\n",
      " 388914.01719589 410787.23716261 381903.48132975 336475.6800798\n",
      " 404859.23273848 468818.77012988 302548.51640814 366520.50588284\n",
      " 324229.40651765 430115.28399999 351761.34887862 394123.77112121\n",
      " 328740.60207079 495284.31266424 399540.65121963 375595.5598953\n",
      " 422128.55738353 440573.12672479 300578.60466212 512808.02090626\n",
      " 388763.96670698 595820.8884778  295095.53921963 439782.28809348\n",
      " 421600.61707245]\n",
      "lasso has been fitted.\n",
      "[437656.72286816 442923.55756182 291894.07140834 324409.2357995\n",
      " 433502.73170584 325644.80104332 365292.26138179 386560.2876291\n",
      " 319078.47885101 483148.58717467 345619.26299228 344032.54856647\n",
      " 348123.45669944 328460.56221483 303474.54955965 364170.14975671\n",
      " 448857.54588331 426577.92039341 441209.57395204 382500.67550111\n",
      " 383509.48996581 291530.42124204 359173.7061074  396640.84687532\n",
      " 304149.46343359 653778.9271052  400323.70809293 352529.32538602\n",
      " 331443.17818582 557549.95401321 547025.5458836  395387.18242383\n",
      " 376339.39654394 404025.87996668 422084.49516532 501646.89409406\n",
      " 257560.42819119 343926.0726257  328394.53567315 313049.79295081\n",
      " 428188.04267918 440084.33480545 466911.55159082 245508.03199237\n",
      " 304088.27026637 738001.73454762 481618.68533624 587962.30638711\n",
      " 555466.31459835 394752.56322115 440683.9123991  450598.02029966\n",
      " 459867.02291745 366696.72822658 464270.04736853 379822.77029279\n",
      " 417842.00051879 398914.78894075 482194.26270402 415057.48131978\n",
      " 334662.76508266 467409.21907351 569022.37600215 371969.80622785\n",
      " 481773.16526509 416831.34255644 559112.02170055 703831.12413962\n",
      " 397092.32406511 405544.69638093 557963.04831484 454019.9521524\n",
      " 479613.51085091 354364.84073522 495431.04506204 438359.38550056\n",
      " 417414.62813995 429001.27792858 281168.0171448  446975.91091006\n",
      " 443895.35906516 380447.50416851 519983.1741414  332691.52852809\n",
      " 609556.3780951  276617.11208493 471223.02726954 275613.63209304\n",
      " 460779.70556814 383799.15483707 470218.35126001 664442.64763538\n",
      " 366160.91302412 423945.73038315 402233.24605637 389730.55901155\n",
      " 301322.49445138 417220.06066804 461799.13947018 471239.01431531\n",
      " 460406.65913365 338938.14907964 445694.76848323 329697.65771117\n",
      " 295015.56602703 291205.77850004 417448.40893384 552632.46836011\n",
      " 399053.32108267 401388.421627   460972.90506674 385349.17452953\n",
      " 453397.63047884 406101.76253476 584146.46913669 542322.76290275\n",
      " 493320.60400523 387094.03122116 504890.13825896 369392.33410772\n",
      " 349474.05358429 482269.5017534  489611.34049469 360580.14398863\n",
      " 449556.82907502 360245.71366809 422479.29775751 301730.06424087\n",
      " 419030.34605472 467337.59442287 776621.33915507 388115.21655569\n",
      " 307686.88705264 354796.97994135 409635.02545133 327731.4359601\n",
      " 396548.80633666 465358.19954186 314602.0658253  523118.88408735\n",
      " 487973.25635542 267082.67519793 523052.11652636 445811.72464412\n",
      " 384775.62352863 339021.89601473 326972.52333215 357343.1326059\n",
      " 361568.7364843  310135.0562218  337095.41187928 377194.97831289\n",
      " 726356.07755866 612889.80418136 456587.10991413 393563.32684468\n",
      " 423875.05721942 500072.42712035 460633.18796636 416646.21086671\n",
      " 455883.52823473 414950.17962462 327067.54838004 424111.76379648\n",
      " 419132.44080317 495801.42502625 345885.99361759 398789.03926147\n",
      " 543878.56297467 316760.9548714  299018.93890075 596344.50149648\n",
      " 323704.02091644 356808.82438934 506958.21511623 414813.17263268\n",
      " 378750.82955844 412449.14713081 387266.63805902 516734.63739811\n",
      " 411264.48376967 286136.6684729  444357.27708745 409847.3923288\n",
      " 542103.32582209 423244.69291158 321496.66685795 326473.48168302\n",
      " 580829.01881075 347671.46118863 506499.91033856 536835.38806285\n",
      " 427671.57940079 349571.11538229 393575.79530028 377158.31327779\n",
      " 365359.54302362 460111.81633338 535997.50361754 288140.6689746\n",
      " 415849.12819348 352210.66587131 387261.27119077 681837.07330763\n",
      " 549018.32839994 360128.94357    517558.04396816 463494.343152\n",
      " 327824.53963342 358220.11840705 334062.69704494 299784.75172732\n",
      " 560640.92867958 384327.66655985 410161.24608253 507996.94899512\n",
      " 432419.2074136  474142.03772589 380020.64595278 479080.80454627\n",
      " 363768.09743585 409222.54351608 387328.15706314 322949.14683228\n",
      " 342179.11597981 401844.48321225 447572.08299455 375619.17213175\n",
      " 496237.49417858 312459.16331848 374763.04463536 276924.63773013\n",
      " 382830.400605   341367.35396891 423723.92458367 445306.24787903\n",
      " 381246.14123891 356343.17559365 358788.04662033 445581.3704724\n",
      " 281027.03762097 435374.18182516 704347.67142478 466699.0242699\n",
      " 302756.38858109 464995.72117083 382763.02880388 417741.15835165\n",
      " 512679.66564601 432399.25931705 331573.86174569 538808.3857766\n",
      " 393594.24813387 480856.69366629 408726.87509515 459930.75961311\n",
      " 406970.00152382 322086.35491576 354172.04439192 484405.12099745\n",
      " 343654.27862838 388508.64599029 500886.64228605 794690.31452873\n",
      " 476443.30511668 310062.9539341  530241.91806718 406898.566942\n",
      " 475639.99783781 366093.96621695 342273.37830563 295667.89825005\n",
      " 324736.95279504 496680.14459055 401548.02419033 328503.5151375\n",
      " 469323.45094453 421582.59539394 426128.40827005 366540.68020827\n",
      " 292155.99211671 377237.08139873 760031.01307617 334029.80493883\n",
      " 342696.55022548 450331.53753231 325743.34199112 519522.63199736\n",
      " 294243.27390437 438514.76810643 380230.2712272  335529.25679278\n",
      " 512133.60827222 454113.30222469 553202.98359361 551558.02317278\n",
      " 390985.32177335 270240.164879   343649.21875675 330330.40030896\n",
      " 518120.82193576 318589.3321355  598383.78865992 512352.69921891\n",
      " 356987.7198174  439489.06743071 378010.7683732  355334.33367127\n",
      " 437005.12171715 354645.69508965 502627.28955775 346939.05464399\n",
      " 387217.72738792 401964.42357123 339479.82721208 461280.77942329\n",
      " 392008.6903113  387237.55714952 498713.88986525 362878.81856351\n",
      " 475474.4803673  294573.22226343 443292.32555684 352102.1357595\n",
      " 485146.58874756 445325.32819858 407473.41811043 416104.90703688\n",
      " 369569.63785244 437685.03524538 335569.98259715 325512.34583374\n",
      " 485158.68551549 611823.12171378 340535.47582658 546404.70032862\n",
      " 373424.65940248 421331.70126908 404124.79174749 281444.0330663\n",
      " 538626.25534825 441481.37636273 487761.86969559 403280.12621811\n",
      " 483113.23591513 333148.40295256 442746.48825725 414809.03584985\n",
      " 389157.46954421 410420.18799627 381202.82523167 337472.59313165\n",
      " 404822.58172541 469096.78580531 302849.08783246 366384.91914943\n",
      " 324275.43175739 429424.85530621 350751.22612659 395047.70995806\n",
      " 328904.61656979 494381.10007993 398121.70610062 375235.2032289\n",
      " 421933.58850716 440594.00182389 301430.68083572 511939.37103583\n",
      " 388621.70337446 595226.68945791 294681.20165783 439115.39308455\n",
      " 421538.81025975]\n",
      "ridge has been fitted.\n",
      "[446713.57455441 423669.45504076 301066.65220094 332047.45274022\n",
      " 420018.61805374 355788.20057028 382202.48176283 373919.44719934\n",
      " 332835.87393877 453108.35603423 368951.52091709 358473.70446825\n",
      " 355057.31040526 343804.84783782 324350.0138253  380183.31486353\n",
      " 454439.62798003 457865.27819086 439752.74482154 382878.18230709\n",
      " 393052.27128768 306157.3990778  365541.45495674 395602.13008647\n",
      " 327019.19613226 614735.51381124 420661.21978607 362718.51018719\n",
      " 345673.18808895 532563.52862239 516462.52419512 400746.89212944\n",
      " 388335.63669677 427662.42156883 436692.34456071 482889.65687946\n",
      " 315920.77853156 338787.59561771 337478.17973991 329542.16170497\n",
      " 421285.25799048 449206.79630314 465336.40038047 275891.57239465\n",
      " 341840.72048741 686995.95561682 469546.05550748 554742.36119743\n",
      " 539576.95559385 405859.55748894 457714.20689063 453972.17690043\n",
      " 454017.6609717  382173.63301672 487311.35811094 416015.92140016\n",
      " 443063.88274    390488.38345443 476070.02180223 391140.07036791\n",
      " 361683.94942896 438785.76301908 578857.12006443 400757.43393366\n",
      " 478500.91086959 426935.06797033 555725.50526405 678876.10401636\n",
      " 409461.3173282  397678.46528698 543857.07213875 463629.94412818\n",
      " 459515.38283723 355233.65878808 519900.9040217  429675.77015879\n",
      " 419066.30716242 415029.02576308 307094.78930564 461000.38911378\n",
      " 445175.70396958 374667.18615295 514118.49559342 343945.68768205\n",
      " 571920.09703275 296513.14793008 473208.22165996 330287.8323123\n",
      " 456004.55909239 380970.57632348 446686.97804141 606589.39284658\n",
      " 377628.34942016 409433.50179228 397979.36083889 404602.45824694\n",
      " 311120.33603158 400070.24125902 458489.41798671 456771.73067729\n",
      " 469366.59056275 355390.09804626 416183.60814182 344156.16060745\n",
      " 313274.1705953  302252.75064704 421454.94336081 520788.67068406\n",
      " 413108.01842495 413261.00939296 454064.99296095 369461.70018849\n",
      " 460914.86382296 400586.55566121 571045.71059541 522309.2274709\n",
      " 496253.88200321 394222.77066948 507490.60379909 384260.59010598\n",
      " 352659.0623603  460508.46945381 478310.45369045 374044.03402074\n",
      " 474225.22488783 376870.54345242 418026.61478421 322358.79077271\n",
      " 426116.43947195 457882.38622975 712558.40643325 377766.61240062\n",
      " 304266.00033018 362262.1872639  409759.06314891 364235.84126586\n",
      " 391671.71288758 471024.63479233 357603.91396486 492825.73898238\n",
      " 459200.74924649 292333.70627363 510829.42971747 455011.62352869\n",
      " 379513.02732215 337284.51466819 343468.30812505 374614.84272655\n",
      " 381826.89937316 323777.96406002 352471.05235928 380025.27773851\n",
      " 668853.89172252 581812.99926787 454305.24393902 367014.88575573\n",
      " 439138.77886519 499539.41769016 469637.64003939 413751.90823356\n",
      " 447863.99084138 407200.65906865 328140.22281483 422325.54110119\n",
      " 418084.75970026 490697.27362576 347340.71513265 412709.23338239\n",
      " 515559.78583811 330608.28599582 319775.13790169 581967.78148865\n",
      " 325642.69304913 359643.5927988  484894.65726359 398721.27283621\n",
      " 389871.2854049  410827.62713791 417422.07217473 507406.55707811\n",
      " 416434.15513696 293716.60096109 436663.45947029 409442.81541794\n",
      " 511585.6442405  407935.18094776 345098.45012856 332742.91399838\n",
      " 575642.05968216 344917.47721025 511043.31869547 513431.73131037\n",
      " 419257.52059114 349419.96366176 422686.09917074 388279.1950622\n",
      " 393767.57315262 461922.39555842 518888.34860294 312009.8425398\n",
      " 422951.01232429 376961.53486931 391964.69356634 642926.70484382\n",
      " 558839.36564353 363890.09917941 504358.53964724 464343.25788557\n",
      " 331787.74429839 369075.49995333 345368.13717107 321878.63287912\n",
      " 559664.71480636 393151.47965486 404556.10136855 494142.52420124\n",
      " 425001.21643172 459033.06568854 374558.42229684 466745.2949078\n",
      " 340309.41763382 409312.74349863 392372.40594195 332744.71752671\n",
      " 351637.59250994 409032.72777172 445685.26324144 374076.98972227\n",
      " 494024.70107985 333351.76191407 390434.21560957 297721.63372387\n",
      " 372230.31966904 366488.17311538 443597.56279679 426297.49998078\n",
      " 370541.78787661 363634.36054014 365371.27371435 451719.57718282\n",
      " 300810.34677548 414884.65129698 660025.34316459 439330.89870033\n",
      " 329467.96325354 472466.78065242 388683.79770384 396447.52766111\n",
      " 520757.89546008 434680.6756075  354764.56458236 519942.32830925\n",
      " 408122.52264531 470208.03967908 417525.79493149 469588.6751189\n",
      " 389690.67529186 338739.43752892 359151.45113874 492028.99684931\n",
      " 352989.60161573 403735.88290329 498020.16364895 746792.54505232\n",
      " 471526.85182506 330506.31798689 508130.80413004 412338.7574585\n",
      " 461606.37848662 369925.44991678 342473.0508786  305554.67366541\n",
      " 315318.20510724 482908.30468323 408308.47010392 321956.5311622\n",
      " 454646.31679976 431231.83395831 442469.25330912 357069.10501022\n",
      " 312200.14635892 392991.14650039 695094.66603726 339114.47620629\n",
      " 362123.8666726  430844.17238289 346150.87815046 497319.52324239\n",
      " 304168.55024453 451975.7653825  370853.47123913 341307.94768464\n",
      " 496577.80266357 443091.08178253 529105.54638664 559749.65118991\n",
      " 398266.8896472  307599.39897214 328823.75665946 358102.24438239\n",
      " 494854.89242747 325862.82327208 595190.62167344 500032.04929494\n",
      " 358024.16082708 419529.28504053 375445.77937244 372572.90822795\n",
      " 433065.97397487 368410.72667877 497984.84903094 363011.97860167\n",
      " 397327.32315931 410891.04387286 353004.43786402 462866.73910971\n",
      " 408123.61452723 398073.0742072  497966.09967706 383163.72943728\n",
      " 476715.16759496 312252.36898813 434620.04089344 352562.97474363\n",
      " 461931.98978806 430261.58461146 396532.99007399 425337.83249631\n",
      " 364273.02956695 439190.52676403 344301.10394176 335287.79893028\n",
      " 474137.475635   587083.97664299 353827.85555933 541054.90925452\n",
      " 396239.50528103 419686.96366515 419177.22161021 306490.1279928\n",
      " 518264.08128134 431367.9650575  492100.98788722 398055.07793407\n",
      " 476812.03175397 345036.93889754 441503.78708861 409543.15428959\n",
      " 387807.17978562 397561.59059389 397345.80786069 366093.54953431\n",
      " 389949.29612782 472928.61188257 328582.56832011 352068.27015489\n",
      " 339093.33358952 422837.68095618 357008.38958027 418307.50737945\n",
      " 331084.46682682 468838.36995749 405775.61417242 374786.30359065\n",
      " 421694.50461743 441231.17689461 326929.87552934 498564.7802737\n",
      " 379673.91578298 554914.93628628 309966.61573996 429196.6385671\n",
      " 425062.06799948]\n",
      "enet has been fitted.\n",
      "[550260.745 457485.275 286461.13  307570.81  395660.19  349888.705\n",
      " 320512.57  424074.215 393472.82  551383.735 287973.065 405308.71\n",
      " 353973.12  356574.305 323191.1   346343.6   462895.98  415956.295\n",
      " 538990.89  337989.6   367011.05  244168.605 384974.825 346088.49\n",
      " 301675.67  652572.075 483478.465 367248.46  363731.525 594648.64\n",
      " 609470.795 280598.805 331695.215 393876.815 467844.76  416046.865\n",
      " 326382.95  333561.5   327387.05  271967.73  464297.795 529049.325\n",
      " 420612.89  247903.945 329833.025 652655.97  570718.99  639957.21\n",
      " 745390.725 436061.135 464602.04  527183.235 515123.135 344753.515\n",
      " 566424.015 373889.095 377515.72  289971.38  602912.055 416645.32\n",
      " 318517.12  438499.435 545013.01  384133.67  518772.165 529735.955\n",
      " 590057.395 499903.96  377848.19  392166.58  592756.46  537072.11\n",
      " 446971.565 317049.015 510701.315 585260.785 472923.89  435406.59\n",
      " 258799.27  488517.575 468112.615 358530.955 446266.33  337438.33\n",
      " 694708.87  300567.105 492952.495 334831.9   505897.83  324636.63\n",
      " 470660.545 705147.875 369914.085 482512.085 386753.88  376017.485\n",
      " 244411.36  371340.68  424485.34  457795.89  491047.515 322814.025\n",
      " 367099.005 319229.065 243523.545 258865.26  373183.215 584265.12\n",
      " 497879.075 411204.665 457165.135 347110.65  512840.4   359488.64\n",
      " 499299.445 542423.39  557852.71  407509.    618180.285 456850.825\n",
      " 257730.25  519296.345 528529.14  346312.665 571799.985 327665.135\n",
      " 396151.185 338360.865 458849.73  391098.56  556919.81  312444.83\n",
      " 232433.385 326881.785 501321.685 347742.535 435420.7   518826.985\n",
      " 354641.85  472856.14  557642.25  282491.48  534724.725 446937.54\n",
      " 378320.625 279197.59  239345.3   354007.225 431090.49  257411.09\n",
      " 321852.055 434798.9   645294.305 588982.995 417215.755 322282.715\n",
      " 436364.255 548493.355 514981.545 375983.335 424201.335 317363.65\n",
      " 264008.595 364854.08  381303.28  638608.135 300088.44  433888.4\n",
      " 549803.12  306159.86  272878.73  617790.42  300398.405 344418.67\n",
      " 490135.81  439969.83  346526.205 499147.815 434144.635 747621.865\n",
      " 309122.37  278453.36  431173.6   368527.15  530916.32  526878.935\n",
      " 271978.51  273257.215 595915.09  286026.225 535994.4   700619.26\n",
      " 496829.42  351902.285 438875.56  330725.73  373876.36  527930.01\n",
      " 711189.94  259749.7   327499.715 395167.18  360738.32  643966.47\n",
      " 546763.8   328966.69  400622.76  474675.015 304734.555 315403.985\n",
      " 284113.385 303782.59  412253.995 538840.92  346196.2   473609.705\n",
      " 460863.485 516604.92  285804.68  467991.655 249376.615 387134.33\n",
      " 396181.095 331881.095 303537.205 497874.83  465945.555 381366.23\n",
      " 492620.075 349756.015 344759.245 255516.415 340854.85  440184.71\n",
      " 454117.255 471954.205 417615.995 390884.42  329345.6   470326.83\n",
      " 269848.47  424666.265 674093.025 340653.315 257364.01  574981.09\n",
      " 304870.145 373985.9   465354.885 493598.96  333210.365 558711.795\n",
      " 555344.23  474912.355 601580.6   499849.48  373481.    304563.365\n",
      " 328251.14  531442.6   361134.305 377321.885 552494.56  586539.945\n",
      " 538993.135 297047.565 605787.645 375506.46  576131.25  319302.155\n",
      " 332084.04  256076.88  259451.18  474384.62  392591.68  276050.22\n",
      " 450774.43  442552.425 450692.915 285969.895 285976.44  352288.625\n",
      " 666204.3   276638.045 301236.96  427547.225 320715.855 564494.55\n",
      " 250116.36  386627.7   285924.59  283553.51  490479.385 389915.91\n",
      " 594845.66  549133.46  400615.71  284381.75  254493.42  328720.47\n",
      " 468731.405 281355.205 559902.68  580483.015 269351.735 423753.825\n",
      " 362616.955 265703.72  422317.335 371337.75  543264.2   267732.96\n",
      " 367736.7   388297.5   261578.555 532990.335 454913.615 535604.335\n",
      " 533907.87  289185.575 529115.92  299287.34  431411.53  305162.395\n",
      " 441617.47  486792.49  445018.455 411517.26  335212.385 418000.565\n",
      " 342162.84  298078.785 413279.05  568662.49  361178.275 528818.73\n",
      " 476892.37  420760.48  408666.875 327080.2   572291.02  405388.095\n",
      " 559976.97  384588.39  429440.055 283511.485 380801.33  394595.82\n",
      " 366746.975 381635.945 321635.535 290647.75  322350.9   604905.95\n",
      " 239468.225 282225.27  337801.71  372561.52  311320.465 385077.595\n",
      " 239133.285 539802.115 357318.065 399389.485 508198.42  385526.95\n",
      " 324959.5   523159.725 285113.08  690957.415 274734.38  402677.535\n",
      " 458093.995]\n",
      "rf has been fitted.\n",
      "[531097.58513268 412090.11196104 276469.10731712 317374.46397653\n",
      " 382881.49563671 340065.326729   357653.10099065 414281.89781196\n",
      " 338436.42579589 577789.4295766  286804.81745223 385403.86116959\n",
      " 332366.89359756 326587.39735391 312802.30106351 344930.15010586\n",
      " 436078.98703245 495912.12289266 539402.80821517 338957.98789888\n",
      " 383609.73076398 255775.33056537 380055.40833179 402103.94825145\n",
      " 289500.79553163 637726.95880506 532756.41294931 371926.66510206\n",
      " 388180.65741376 549222.48103101 593374.24493775 313053.48709295\n",
      " 337940.08619764 435949.11750441 442313.00624703 472870.15637943\n",
      " 313444.63296177 307210.70629082 352717.29796555 276025.47521536\n",
      " 448573.56805381 501416.02136463 454993.98110477 269352.1348481\n",
      " 336690.92946058 659318.44044896 538609.88782802 650148.70200854\n",
      " 609475.26756145 440388.01062459 476195.1049416  517518.36507591\n",
      " 543700.47206895 338645.98353392 542619.02149274 383349.95848968\n",
      " 389619.28083782 313409.45959821 558014.6258135  395989.8059703\n",
      " 343419.78136029 455647.18927672 457540.33952179 403142.09749159\n",
      " 516542.61586375 516019.34988791 588243.081176   552459.95637496\n",
      " 384404.88678525 380640.23304536 515560.23922406 522488.14368944\n",
      " 455439.25159197 321782.96032351 518294.3303651  540344.06348194\n",
      " 422399.68172141 470364.26656838 276604.67695497 505774.37896167\n",
      " 462000.55382622 332052.53460628 474724.66416626 320431.0250822\n",
      " 648018.5116026  311294.45620374 545531.63696031 307011.0072663\n",
      " 532684.02621432 336092.88431211 454302.81247223 674919.96624431\n",
      " 370086.98608003 461885.97671623 390834.99179792 376755.8079782\n",
      " 256882.67334057 372138.31970351 456736.47132225 443824.62537935\n",
      " 501324.1562384  306519.15169125 381108.08798613 336897.41833249\n",
      " 262146.93552221 273811.19886343 410448.9222874  608841.96080559\n",
      " 457414.20424596 409420.04657163 434034.81478639 340009.55603683\n",
      " 477724.27140504 354691.45822071 517273.48178847 531446.74530438\n",
      " 537348.32422668 413280.37282669 532441.38162435 442345.86667991\n",
      " 267987.63191318 504736.00573002 528112.457542   346522.54825605\n",
      " 566741.3286207  326166.6869697  409334.2942753  338629.35009143\n",
      " 457055.12387156 441880.59232446 606916.70156068 328147.50523254\n",
      " 263842.10532275 335396.47332458 457787.30826266 352867.91091864\n",
      " 422531.36447546 527080.60338724 361162.968181   473311.21753051\n",
      " 575678.19143211 277165.75892887 493310.75292469 511898.87152818\n",
      " 385527.64785    299740.08313087 267202.50336193 349034.8059104\n",
      " 384348.86556115 274622.91095412 343171.28310146 401037.67550423\n",
      " 617403.49659625 562747.37067138 406900.07492361 325137.44024561\n",
      " 461861.32413087 490253.83728067 564053.43384326 372726.62446778\n",
      " 447056.59361308 331216.98878105 278453.21979162 388658.05524671\n",
      " 381381.41734121 596963.68747059 306581.83421695 406706.04742572\n",
      " 522423.95453562 312585.30370619 295181.58861139 573249.72699552\n",
      " 287384.3693872  370194.36092431 522611.35624534 399711.53842681\n",
      " 385703.32945573 449505.18971372 413285.85007093 679863.64331624\n",
      " 324588.75023032 288360.00790294 452538.19303069 337026.21244085\n",
      " 554687.74864148 537266.04047595 295236.4074105  277114.76167063\n",
      " 559352.06484022 289655.24819768 512083.71034519 638582.50582051\n",
      " 448913.9054432  344092.08199566 431855.55737098 322802.89717466\n",
      " 388167.88219937 516017.94908153 665335.67855288 263532.65776427\n",
      " 340807.05123202 379101.01742374 391366.97389303 622556.48284024\n",
      " 555815.87892639 332655.53381075 472878.83289402 430979.0665779\n",
      " 308788.28357403 321772.76334497 294298.71337485 294274.25870286\n",
      " 425354.61335391 524053.34201462 391047.00861382 450336.82989125\n",
      " 449080.1263998  554203.75013222 329078.28938377 433864.11939822\n",
      " 257128.41603603 377373.04732468 376431.79205791 319771.32048256\n",
      " 321249.9726216  455192.84078325 382234.41742933 387962.96405384\n",
      " 535090.97407703 337653.94574637 332339.42365534 277646.19773216\n",
      " 337723.296817   448575.48475051 443547.42511262 445183.09846816\n",
      " 396890.96296031 368910.80351267 334420.21634018 443793.09122701\n",
      " 284649.18734932 375056.63529454 652864.22662485 387720.22828844\n",
      " 284154.21434409 502312.15403019 320294.82351897 377966.08521222\n",
      " 523780.62188129 433688.80555538 342742.80478619 598646.2483226\n",
      " 551403.2724542  472676.1613533  534221.51756479 489812.88959859\n",
      " 377607.69751769 340550.86195172 334798.22072379 531364.35424277\n",
      " 380495.76539497 391199.6663649  583346.27126564 631948.92291789\n",
      " 502595.95112046 298090.51671092 582664.14619046 385891.65771478\n",
      " 578166.98610865 333876.74225242 383763.26691868 271798.57508563\n",
      " 266377.56288982 441929.78740894 375064.62183195 301688.2386855\n",
      " 449716.46289196 428515.88045515 441830.53503803 294651.58935986\n",
      " 276180.83919727 391869.27069253 660368.67603578 278387.3730143\n",
      " 318460.40021121 390359.28653253 333243.97175589 588522.43063773\n",
      " 260379.040842   458849.61156226 292652.06059204 277503.91096248\n",
      " 524416.1636611  398309.69712994 570189.39046493 527203.61182898\n",
      " 393465.83971942 293513.96597035 267963.1772412  345141.52774856\n",
      " 487618.52458897 286002.86402739 542479.30602671 584024.42278842\n",
      " 283000.78521539 414300.47681037 353771.41456704 299769.87740522\n",
      " 422927.4751472  411335.48043813 509699.04937971 310118.99859959\n",
      " 376227.34732855 369805.75583414 278379.67286097 557289.74365347\n",
      " 446724.32681364 522458.13179298 515176.72832861 291451.12309216\n",
      " 526774.77491038 300106.72990703 407131.08536005 303488.96805629\n",
      " 447638.76850545 463697.4484921  406156.33619068 401260.17643583\n",
      " 335578.38568393 386835.97257598 335140.62595893 296682.81851898\n",
      " 437609.48606353 555745.04406468 375049.95110355 553440.315637\n",
      " 473095.01083458 445323.57643063 412442.22924837 295842.59078855\n",
      " 550358.84772703 391366.97389303 561020.61922267 388238.05065702\n",
      " 426550.26377773 302922.87911559 443418.57173553 392651.18148959\n",
      " 387967.84777879 384348.86556115 322991.85181769 321306.78673043\n",
      " 324497.70401555 593894.78370943 266449.71310073 277389.40014605\n",
      " 345398.61753397 360838.05548683 329595.63528062 399923.90031111\n",
      " 262357.95778101 517562.2201752  372006.61292632 387604.02763568\n",
      " 456516.43961527 395461.08791747 330651.997496   535743.76213824\n",
      " 305740.86801732 650172.67897864 274953.59345534 422231.65560576\n",
      " 445097.7181823 ]\n",
      "gb has been fitted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "for name, model in fitted_models.items():\n",
    "    try:\n",
    "        pred = model.predict(X_test)\n",
    "        print(pred)\n",
    "        print(name, 'has been fitted.')\n",
    "        \n",
    "    except NotFittedError as e:\n",
    "        print(repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nuULcFh-aRll"
   },
   "source": [
    "Nice. Now we're ready to evaluate how our models performed!\n",
    "\n",
    "<hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "\n",
    "<div style=\"text-align:center; margin: 40px 0 40px 0;\">\n",
    "    \n",
    "[**Back to Contents**](#toc)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDBceRCUaRln"
   },
   "source": [
    "<br id=\"evaluate\">\n",
    "\n",
    "# 5. Evaluate models and select winner\n",
    "\n",
    "### Finally, it's time to evaluate our models and pick the best one.\n",
    "\n",
    "<br>\n",
    "Let's display the holdout $R^2$ score for each fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjZIxUywaRlp",
    "outputId": "081a9128-ac56-4414-88e7-9d5967f25daa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso 0.30744115883069695\n",
      "ridge 0.31550675360698766\n",
      "enet 0.3422914802767902\n",
      "rf 0.4837169608650216\n",
      "gb 0.48743188672113513\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_ for each fitted model\n",
    "for name, model in fitted_models.items():\n",
    "    print(name, model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LrCB6hJjaRl_"
   },
   "source": [
    "You should see something similar to the below scores:\n",
    "\n",
    "  \n",
    "    lasso 0.309321321129\n",
    "    ridge 0.316805719351\n",
    "    enet 0.342759786956\n",
    "    rf 0.480576134721\n",
    "    gb 0.48873808731\n",
    "\n",
    "\n",
    "If your numbers are way off, check to see if you've set the <code style=\"color:steelblue\">random_state=</code> correctly for each of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLkgmi5GaRmD"
   },
   "source": [
    "Next, import the <code style=\"color:steelblue\">r2_score()</code> and <code style=\"color:steelblue\">mean_absolute_error()</code> functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XDMzBAIwaRmE"
   },
   "outputs": [],
   "source": [
    "# Import r2_score and mean_absolute_error functions\n",
    "from sklearn.metrics import r2_score \n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9x7ov6vxaRmH"
   },
   "source": [
    "### Let's see how the fitted models perform on our test set!\n",
    "\n",
    "<br>\n",
    "First, access your fitted random forest and display the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y3gVlv16aRmK",
    "outputId": "a477f376-da47-4350-c320-2788ecd9a446"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "  ...rs='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=123, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'randomforestregressor__n_estimators': [100, 200], 'randomforestregressor__max_features': ['auto', 'sqrt', 0.33]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display fitted random forest object\n",
    "fitted_models['rf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9-7YmMxQaRmX"
   },
   "source": [
    "Predict the test set using the fitted random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5zcwitX0aRmY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "# Predict test set using fitted random forest\n",
    "pred = fitted_models['rf'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ibmyfd9naRmb"
   },
   "source": [
    "Finally, we use the scoring functions we imported to calculate and print $R^2$ and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Denr3VvaRmc",
    "outputId": "c3158d81-8ebd-496d-fd98-bbc8c17ac19f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  0.5666046230135338\n",
      "MAE:  68470.95126005363\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print R^2 and MAE\n",
    "print('R^2: ', r2_score(y_test, pred))\n",
    "print('MAE: ', mean_absolute_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DSIt5nRZaRmg"
   },
   "source": [
    "In the next exercise, we'll evaluate all of our fitted models on the test set and pick the winner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZJnA2EoQaRmh"
   },
   "source": [
    "<br><hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "## <span style=\"color:RoyalBlue\">Exercise 5.5</span>\n",
    "\n",
    "**Use a <code style=\"color:SteelBlue\">for</code> loop, print the performance of each model in <code style=\"color:SteelBlue\">fitted_models</code> on the test set.**\n",
    "* Print both <code style=\"color:SteelBlue\">r2_score</code> and <code style=\"color:SteelBlue\">mean_absolute_error</code>.\n",
    "* Those functions each take two arguments:\n",
    "    * The actual values for your target variable (<code style=\"color:SteelBlue\">y_test</code>)\n",
    "    * Predicted values for your target variable\n",
    "* Label the output with the name of the algorithm. For example:\n",
    "\n",
    "<pre>\n",
    "lasso\n",
    "--------\n",
    "R^2: 0.409313458932\n",
    "MAE: 84963.5598922\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5UZ_NMU3aRmi",
    "outputId": "b7e08122-a27a-455d-d1d3-489bda1738b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso\n",
      "R^2:  0.4093410739690314\n",
      "MAE:  84957.97844920789\n",
      "===================================\n",
      "ridge\n",
      "R^2:  0.40978386776640274\n",
      "MAE:  84899.82281275438\n",
      "===================================\n",
      "enet\n",
      "R^2:  0.40415614629545416\n",
      "MAE:  86465.82558534491\n",
      "===================================\n",
      "rf\n",
      "R^2:  0.5666046230135338\n",
      "MAE:  68470.95126005363\n",
      "===================================\n",
      "gb\n",
      "R^2:  0.5416475698153993\n",
      "MAE:  70505.20969788785\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n",
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    }
   ],
   "source": [
    "# Code here\n",
    "for name, model in fitted_models.items(): \n",
    "    pred_var = model.predict(X_test)\n",
    "    print(name)\n",
    "    print('R^2: ', r2_score(y_test, pred_var))\n",
    "    print('MAE: ', mean_absolute_error(y_test, pred_var))\n",
    "    print('===================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CCIaDqL7aRnE"
   },
   "source": [
    "**Next, ask yourself these questions to pick the winning model:**\n",
    "* Which model had the highest $R^2$ on the test set?\n",
    "\n",
    "> Random forest\n",
    "\n",
    "* Which model had the lowest mean absolute error?\n",
    "\n",
    "> Random forest\n",
    "\n",
    "* Are these two models the same one?\n",
    "\n",
    "> Yes\n",
    "\n",
    "* Did it also have the best holdout $R^2$ score from cross-validation?\n",
    "\n",
    "> Yes\n",
    "\n",
    "* **Does it satisfy our win condition?**\n",
    "\n",
    "> Yes, its mean absolute error is less than \\$70,000!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27Zt4myMaRnG"
   },
   "source": [
    "**Finally, let's plot the performance of the winning model on the test set. Run the code below.**\n",
    "* It first plots a scatter plot.\n",
    "* Then, it plots predicted transaction price on the X-axis.\n",
    "* Finally, it plots actual transaction price on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_4ZA0denaRnG",
    "outputId": "b1d3ea22-4227-4e4f-a66f-07e449330581"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamal\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gb_pred = fitted_models['rf'].predict(X_test)\n",
    "plt.scatter(gb_pred, y_test)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-p1GTfTCaRnO"
   },
   "source": [
    "This last visual check is a nice way to confirm our model's performance.\n",
    "* Are the points scattered around the 45 degree diagonal?\n",
    "\n",
    "<br>\n",
    "<hr style=\"border-color:royalblue;background-color:royalblue;height:1px;\">\n",
    "\n",
    "<div style=\"text-align:center; margin: 40px 0 40px 0;\">\n",
    "    \n",
    "[**Back to Contents**](#toc)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-no7U62aRnP"
   },
   "source": [
    "<br>\n",
    "\n",
    "### Finally, let's save the winning model.\n",
    "\n",
    "Great job! You've created a pretty kick-ass model for real-estate valuation. Now it's time to save your hard work.\n",
    "\n",
    "First, let's take a look at the data type of your winning model.\n",
    "\n",
    "***Run each code cell below after completing the exercises above.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-m28MLAsaRnQ",
    "outputId": "f9a16334-4ae4-4ab2-b21a-d96f347fe124"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.model_selection._search.GridSearchCV"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fitted_models['rf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sWc5UnHCaRnT"
   },
   "source": [
    "It looks like this is still the <code style=\"color:steelblue\">GridSearchCV</code> data type. \n",
    "* You can actually directly save this object if you want, because it will use the winning model pipeline by default. \n",
    "* However, what we really care about is the actual winning model <code style=\"color:steelblue\">Pipeline</code>, right?\n",
    "\n",
    "In that case, we can use the <code style=\"color:steelblue\">best\\_estimator_</code> method to access it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M83BEVxSaRna",
    "outputId": "f96de2a4-e7e8-4b65-dd5c-5c430920b634"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.pipeline.Pipeline"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fitted_models['rf'].best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1vvNp2AaRnf"
   },
   "source": [
    "If we output that object directly, we can also see the winning values for our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqQ4vZqQaRng",
    "outputId": "7873c860-913f-4029-ccdf-e470e994cd4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "           oob_score=False, random_state=123, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_models['rf'].best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7bkJYTqaRnj"
   },
   "source": [
    "See? The winning values for our hyperparameters are:\n",
    "* <code style=\"color:steelblue\">n_estimators: <span style=\"color:crimson\">200</span></code>\n",
    "* <code style=\"color:steelblue\">max_features : <span style=\"color:crimson\">'auto'</span></code>\n",
    "\n",
    "Great, now let's import a helpful package called <code style=\"color:steelblue\">pickle</code>, which saves Python objects to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTbGKUq7aRnk"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mOlCR6YaaRn5"
   },
   "source": [
    "Let's save the winning <code style=\"color:steelblue\">Pipeline</code> object into a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IstjfSW8aRn5"
   },
   "outputs": [],
   "source": [
    "with open('saved_models/final_model_employee.pkl', 'wb') as f:\n",
    "    pickle.dump(fitted_models['rf'].best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-kOHDtmCaRn-"
   },
   "source": [
    "Congratulations... you've built and saved a successful model trained using machine learning!\n",
    "\n",
    "As a reminder, here are a few things you did in this module:\n",
    "* You split your dataset into separate training and test sets.\n",
    "* You set up preprocessing pipelines.\n",
    "* You tuned your models using cross-validation.\n",
    "* And you evaluated your models, selecting and saving the winner.\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "<div style=\"text-align:center; margin: 40px 0 40px 0;\">\n",
    "    \n",
    "[**Back to Contents**](#toc)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOdbXFBAaRoA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lesson 4 - Real Estate Model Training.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
